# Independence

## Required packages

The following CRAN packages must be installed:

| Required CRAN Packages |
|------------------------|
| tidyverse              |
| usethis                |
| janitor                |
| skimr                  |
| tidymodels             |
| survey                |
| olsrr                |




## Page 140 Independence/Dependence with Means

```{r}
#| eval: false

library(usethis) # use_github_file 
library(tidyverse) # read_csv()
library(janitor) # clean_names() 
library(skimr) # skim()
library(tidymodels) #tidy() glance()
library(survey) 
```

```{r}
#| include: false

library(usethis) # use_github_file 
library(tidyverse) # read_csv()
library(janitor) # clean_names() 
library(skimr) # skim()
library(tidymodels) #tidy() glance()
library(survey) 
```

### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/MultiLevel.csv",
                save_as = "data_multilevel.csv")

data_multilevel <- read_csv("data_multilevel.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/MultiLevel.csv",
                save_as = "data_multilevel.csv")
```

### Load data from your computer

```{r}
#| eval: false
data_multilevel <- read_csv("data_multilevel.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
data_multilevel %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(data_multilevel)
```

### Specify Design

We indicate town is a grouping variable (idcomm which I think stands for id community).

```{r}
clus_multilevel <- svydesign(ids = ~idcomm,
                             data = data_multilevel)

summary(clus_multilevel)

```

### Calculate Mean/SE 


In the calculation we below we us the is.na() command. This provides a vector of TRUE/FALSE non-missing values. TRUE indicates missing. We using an exclamation mark in from to the INVERT the TRUE/FALSE. That means, TRUE indicates NOT missing after we using the exclamation mark. Then we use the sum() command to add up the TRUEs. Each TRUE counts as 1 so this provides us with a count of non-missing values. We could have used the n() command but this doesn't know if there are missing values and can produce incorrect results.


```{r}
data_multilevel %>%
  summarise(n = sum(!is.na(commlength)),
            mean = mean(commlength),
            SD= sd(commlength),
            SE = SD/sqrt(n)) %>%
  as.data.frame()

```

Consider the SE calculation here. It is the usual one.

$$
SE = \frac{SD}{\sqrt{n}}= \frac{22.44965}{\sqrt{9859}} = 0.2260962
$$


### Calculate Mean/SE Incorporting Cluster Dependence

```{r}
svymean(~commlength,
        design = clus_multilevel)

```


Notice how the SE is much larger when clustering is taken into account. When the clusters are taken into account it means that we realize that within a clusters each person does not provide information independent of the other people. So from a statistical perspective this makes the effective sample size smaller when calculating SE. In this data set the 9859 people (with dependence due to clusters) are equivalent to 1796.219 independent people - see the calculation below. Note though this is not the actual calculation "under the hood", it is just a means of illustrating what we are compensating for when we take clusters into account. The lower effective sample size (due to a lack of independence) results in a larger Standard Error (0.5297 vs 0.2261).

$$
SE = \frac{SD}{\sqrt{n}}= \frac{22.44965}{\sqrt{1796.219}} = 0.5296999
$$



## Page 142 Independence/Dependence with Regression

### Regular Regression No Adjustment

```{r}
lm8_1 <- lm(income ~ male + married, 
            data = data_multilevel)
```


```{r}
#| eval: false
tidy(lm8_1)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_1))
```


```{r}
confint(lm8_1)
```


```{r}
#| eval: false
glance(lm8_1)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_1))

```




### Dependence Adjusted Regression



```{r}
clus_multilevel <- svydesign(ids = ~idcomm,
                             data = data_multilevel)

summary(clus_multilevel)

```


```{r}
lm8_2 <- svyglm(income ~ male + married, 
                design = clus_multilevel)
```


```{r}
#| eval: false
tidy(lm8_2)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_2))
```


```{r}
confint(lm8_2)
```


```{r}
#| eval: false
glance(lm8_2)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_2))

```

### Comparing results

In the two outputs below notice the standard error is higher in the Dependence Adjusted results. This follows the same principle as when we dealt with means: when dependence is taken into account the standard error is larger. This correspondingly affects p-values - making them larger (further from 0) but more accurate.

#### Regular regression - see SE for weights

```{r}
#| eval: false
tidy(lm8_1)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_1))
```

#### Dependence Adjusted - see SE for weights


```{r}
#| eval: false
tidy(lm8_2)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_2))
```

## Serial Correlation

### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Colombia2000_16.csv",
                save_as = "columbia.csv")

columbia <- read_csv("columbia.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Colombia2000_16.csv",
                save_as = "columbia.csv")
```

### Load data from your computer

```{r}
#| eval: false
columbia <- read_csv("columbia.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
columbia %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(columbia)
```



### Regular Regression


```{r}
lm8_3 <- lm(homicide_rate ~ poverty,
            data=columbia)
```



```{r}
#| eval: false
tidy(lm8_3)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_3))
```


```{r}
confint(lm8_3)
```


```{r}
#| eval: false
glance(lm8_3)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_3))

```

#### Check residuals
```{r}
#| include: false
library(olsrr)
```



```{r}
library(olsrr)

# Deleted studentized residual vs fitted values plot
ols_plot_resid_stud_fit(lm8_3)
```


Obtain the residuals using the augment() command from the broom package. The residuals are in the .resid column.

```{r}
#| eval: false
augment(lm8_3)
```

```{r}
#| echo: false
lm8_3aug <- augment(lm8_3)
knitr::kable(lm8_3aug, align = rep("r",8))
```

We can calculate the Durbin-Watson test statistic with the formula below. But that notation can be hard to understand.

$$
d = \frac{\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2}{\sum\limits_{ti=1}^n\epsilon_{ti}^2}
$$

Let's consider the numerator first. Look the values in the .resid column and see how they are used.

$$
\begin{aligned}
\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2 &= (2.375043 - 9.849052)^2 + (12.953023-2.375043)^2 + (1.091291-12.953023)^2 + ... + (3.729203 - 4.047614) ^2\\
&= 438.059
\end{aligned}
$$


Now the denominator:

$$
\begin{aligned}
\sum\limits_{ti=1}^n\epsilon_{ti}^2 &= 9.849052^2 +  2.375043^2 + 12.953023^2 + ... + 3.729203^2\\
&= 629.6975
\end{aligned}
$$

We combine them to get the final Durbin-Watson statistic:

$$
d = \frac{\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2}{\sum\limits_{ti=1}^n\epsilon_{ti}^2} = \frac{438.059}{629.6975} = 0.6956658
$$

