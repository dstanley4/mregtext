# Independence

## Required packages

The following CRAN packages must be installed:

| Required CRAN Packages |
|------------------------|
| tidyverse              |
| usethis                |
| janitor                |
| skimr                  |
| tidymodels             |
| survey                 |
| olsrr                  |
| car                    |
| prais                  |
| gee                    |




## Page 140 Independence/Dependence with Means

```{r}
#| eval: false

library(usethis) # use_github_file 
library(tidyverse) # read_csv()
library(janitor) # clean_names() 
library(skimr) # skim()
library(tidymodels) #tidy() glance()
library(survey) 
```

```{r}
#| include: false

library(usethis) # use_github_file 
library(tidyverse) # read_csv()
library(janitor) # clean_names() 
library(skimr) # skim()
library(tidymodels) #tidy() glance()
library(survey) 
```

### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/MultiLevel.csv",
                save_as = "data_multilevel.csv")

data_multilevel <- read_csv("data_multilevel.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/MultiLevel.csv",
                save_as = "data_multilevel.csv")
```

### Load data from your computer

```{r}
#| eval: false
data_multilevel <- read_csv("data_multilevel.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
data_multilevel %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(data_multilevel)
```

### Specify Design

We indicate town is a grouping variable (idcomm which I think stands for id community).

```{r}
clus_multilevel <- svydesign(ids = ~idcomm,
                             data = data_multilevel)

summary(clus_multilevel)

```

### Calculate Mean/SE 


In the calculation we below we us the is.na() command. This provides a vector of TRUE/FALSE non-missing values. TRUE indicates missing. We using an exclamation mark in from to the INVERT the TRUE/FALSE. That means, TRUE indicates NOT missing after we using the exclamation mark. Then we use the sum() command to add up the TRUEs. Each TRUE counts as 1 so this provides us with a count of non-missing values. We could have used the n() command but this doesn't know if there are missing values and can produce incorrect results.


```{r}
data_multilevel %>%
  summarise(n = sum(!is.na(commlength)),
            mean = mean(commlength),
            SD= sd(commlength),
            SE = SD/sqrt(n)) %>%
  as.data.frame()

```

Consider the SE calculation here. It is the usual one.

$$
SE = \frac{SD}{\sqrt{n}}= \frac{22.44965}{\sqrt{9859}} = 0.2260962
$$


### Calculate Mean/SE Incorporting Cluster Dependence

```{r}
svymean(~commlength,
        design = clus_multilevel)

```


Notice how the SE is much larger when clustering is taken into account. When the clusters are taken into account it means that we realize that within a clusters each person does not provide information independent of the other people. So from a statistical perspective this makes the effective sample size smaller when calculating SE. In this data set the 9859 people (with dependence due to clusters) are equivalent to 1796.219 independent people - see the calculation below. Note though this is not the actual calculation "under the hood", it is just a means of illustrating what we are compensating for when we take clusters into account. The lower effective sample size (due to a lack of independence) results in a larger Standard Error (0.5297 vs 0.2261).

$$
SE = \frac{SD}{\sqrt{n}}= \frac{22.44965}{\sqrt{1796.219}} = 0.5296999
$$



## Page 142 Independence/Dependence with Regression

### Regular Regression No Adjustment

```{r}
lm8_1 <- lm(income ~ male + married, 
            data = data_multilevel)
```


```{r}
#| eval: false
tidy(lm8_1)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_1))
```


```{r}
confint(lm8_1)
```


```{r}
#| eval: false
glance(lm8_1)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_1))

```




### Dependence Adjusted Regression



```{r}
clus_multilevel <- svydesign(ids = ~idcomm,
                             data = data_multilevel)

summary(clus_multilevel)

```


```{r}
lm8_2 <- svyglm(income ~ male + married, 
                design = clus_multilevel)
```


```{r}
#| eval: false
tidy(lm8_2)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_2))
```


```{r}
confint(lm8_2)
```


```{r}
#| eval: false
glance(lm8_2)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_2))

```

### Comparing results

In the two outputs below notice the standard error is higher in the Dependence Adjusted results. This follows the same principle as when we dealt with means: when dependence is taken into account the standard error is larger. This correspondingly affects p-values - making them larger (further from 0) but more accurate.

#### Regular regression - see SE for weights

```{r}
#| eval: false
tidy(lm8_1)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_1))
```

#### Dependence Adjusted - see SE for weights


```{r}
#| eval: false
tidy(lm8_2)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_2))
```

## Page 145 Serial Correlation

### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Colombia2000_16.csv",
                save_as = "columbia.csv")

columbia <- read_csv("columbia.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Colombia2000_16.csv",
                save_as = "columbia.csv")
```

### Load data from your computer

```{r}
#| eval: false
columbia <- read_csv("columbia.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
columbia %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(columbia)
```



### Regular Regression


```{r}
lm8_3 <- lm(homicide_rate ~ poverty,
            data=columbia)
```



```{r}
#| eval: false
tidy(lm8_3)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_3))
```


```{r}
confint(lm8_3)
```


```{r}
#| eval: false
glance(lm8_3)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_3))

```

#### Check Autocorrelation of Residuals
```{r}
#| include: false
library(olsrr)
```



```{r}
library(olsrr)

# Deleted studentized residual vs fitted values plot
ols_plot_resid_stud_fit(lm8_3)
```


Obtain the residuals using the augment() command from the broom package. The residuals are in the .resid column.

```{r}
#| eval: false
augment(lm8_3)
```

```{r}
#| echo: false
lm8_3aug <- augment(lm8_3)
knitr::kable(lm8_3aug, align = rep("r",8))
```

We can calculate the Durbin-Watson test statistic with the formula below. But that notation can be hard to understand.

$$
d = \frac{\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2}{\sum\limits_{ti=1}^n\epsilon_{ti}^2}
$$

Let's consider the numerator first. Look the values in the .resid column and see how they are used.

$$
\begin{aligned}
\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2 &= (2.375043 - 9.849052)^2 + (12.953023-2.375043)^2 + (1.091291-12.953023)^2 + ... + (3.729203 - 4.047614) ^2\\
&= 438.059
\end{aligned}
$$


Now the denominator:

$$
\begin{aligned}
\sum\limits_{ti=1}^n\epsilon_{ti}^2 &= 9.849052^2 +  2.375043^2 + 12.953023^2 + ... + 3.729203^2\\
&= 629.6975
\end{aligned}
$$

We combine them to get the final Durbin-Watson statistic:

$$
d = \frac{\sum\limits_{ti=2}^n(\epsilon_{ti}-\epsilon_{ti-1})^2}{\sum\limits_{ti=1}^n\epsilon_{ti}^2} = \frac{438.059}{629.6975} = 0.6956658
$$

Compare this to the output:

```{r}
library(car)
durbinWatsonTest(lm8_3)
```

This output also contains an autocorrelation value. It is calculated using the formula below. Not this is the formula used by R. The formula in the text book on p. 149 Footnote 10 is not used by the R durbinWatsonTest() command.

$$
r = \frac{\sum\limits_{ti=2}^n\epsilon_{ti}\epsilon_{ti-1}}{\sum\limits_{ti=1}^n\epsilon_{ti}^2} = 0.5641004
$$


Autocorrelation of residuals is a way of determining if the independence of rows assumption for regression is true.
The autocorrelation of residuals is a way of examining if the residuals at one time are related to the residuals at the next time. If the two are unrelated then the rows are independent - and the regression assumption is true. Alternatively, if the residuals at one time are related to residuals at the next time - then the rows are dependent - and the regression assumption is violated.

Consider the table below. The first column on the left is the original residuals. The second column on the right is the residuals shifted by one spot (i.e., a lag of 1 time period). Autocorrelation of residuals checks to see if these two columns are correlated. If the residuals are correlated (i.e., the columns below are correlated) this indicates the errors at one time are related to the errors at another time. That is, the errors on one row are related to the errors on another row. If this is true, the rows are not independent of each other and the regression assumption is violated.

The textbook autcorrelation formula would give you the actual correlation between the two columns below which is .62. For technical reasons R actually uses a slightly different calculation formula, above, which is conceptually similar, but produces a value of 0.56.


```{r}
#| echo: false
all_residuals <- lm8_3aug$.resid
r1 <- all_residuals[2:17]
r2 <- all_residuals[1:16]

residdf <- data.frame(residuals = r2, shifted_residuals = r1)

knitr::kable(residdf, align = rep("r",2))
```

## Page 150 Corrections for dependence

### Regular regression

```{r}
lm8_4 <- lm(homicide_rate ~ poverty,
            data = columbia)

```


```{r}
#| eval: false
tidy(lm8_4)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_4))
```


```{r}
confint(lm8_4)
```


```{r}
#| eval: false
glance(lm8_4)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_4))

```


### Page Prais-Winsten Regression Model

```{r}
library(prais)

lm8_5 <- prais_winsten(homicide_rate ~ poverty,
                       data = columbia,
                       index = "index")

summary(lm8_5)
```



## Page 153 GEE for Longitudinal Data


### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Esteem.csv",
                save_as = "data_esteem.csv")

data_esteem <- read_csv("data_esteem.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/Esteem.csv",
                save_as = "data_esteem.csv")
```

### Load data from your computer

```{r}
#| eval: false
data_esteem <- read_csv("data_esteem.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
data_esteem %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(data_esteem)
```

```{r}
head(data_esteem)
```


### Regular Regression

```{r}
lm8_6 <- lm(esteem ~ cohesion + stress,
            data = data_esteem)
```


```{r}
#| eval: false
tidy(lm8_6)
```

```{r}
#| eval: true
#| echo: false
knitr::kable(tidy(lm8_6))
```


```{r}
confint(lm8_6)
```


```{r}
#| eval: false
glance(lm8_6)

```

```{r}
#| eval: true
#| echo: false
knitr::kable(glance(lm8_6))

```

## Page 154 GEE Regression AR(1) Pattern

```{r}
library(gee)

lm8_7 <- gee(esteem ~ cohesion + stress,
             id = newid,
             data = data_esteem,
             corstr = "AR-M",
             Mv = 1)

summary(lm8_7)

```

## Page 155 GEE Regression Unstructured Pattern

```{r}

lm8_8 <- gee(esteem ~ cohesion + stress,
             id = newid,
             data = data_esteem,
             corstr = "unstructured")

summary(lm8_8)

```

## Page 157 

## Page 162 Chapter Exercises

### Obtain data and save it to your computer

```{r}
#| include: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/USCounties2010.csv",
                save_as = "data_uscounties.csv")

data_counties <- read_csv("data_uscounties.csv", show_col_types = FALSE) %>% clean_names()
```

```{r}
#| eval: false
use_github_file(repo_spec = "https://github.com/johnhoffmannVA/LinearRegression/blob/main/USCounties2010.csv",
                save_as = "data_uscounties.csv")
```

### Load data from your computer

```{r}
#| eval: false
data_counties <- read_csv("data_uscounties.csv") %>% 
  clean_names()
```

### Inspect data

```{r}
data_counties %>% 
  glimpse()  
```


### View data

Remember you can always use the view() command in R to see the data in a spreadsheet:

```{r}
#| eval: false
view(data_counties)
```


