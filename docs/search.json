[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Textbook Supplement",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to make it easier use R code with the textbook examples. In some cases I provide extra R code to make life easier for you (e.g., obtaining data from GitHub). In other cases, I use different code from the book to maintain the code-style we use in the course (i.e., a tidyverse approach to R)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "No R-code in this chapter."
  },
  {
    "objectID": "chapter2.html#required-packages",
    "href": "chapter2.html#required-packages",
    "title": "2  Review",
    "section": "2.1 Required packages",
    "text": "2.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\npsych\n\n\n\nREMINDER: Never use the command library(psych)."
  },
  {
    "objectID": "chapter2.html#page-12-normal-density-plot",
    "href": "chapter2.html#page-12-normal-density-plot",
    "title": "2  Review",
    "section": "2.2 Page 12 Normal Density Plot",
    "text": "2.2 Page 12 Normal Density Plot\n\n2.2.1 Normal Distribution\n\nlibrary(tidyverse)\n\npopulation_data &lt;- data.frame(weights = rnorm(1000000, 80, 10))\n\nweight_graph &lt;- ggplot(data = population_data,\n                   mapping = aes(x = weights)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Weights\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n\nprint(weight_graph)\n\n\n\n\n\n\n2.2.2 Skewed Distribution\n\nlibrary(tidyverse)\n\nincome_data &lt;- data.frame(income = rf(1000000, df1 = 5, df2 = 2000))\n\nincome_graph &lt;- ggplot(data = income_data,\n                   mapping = aes(x = income)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Income\", \n                     breaks = seq(0,  6, by = 2)) +\n  scale_y_continuous(name = \"Density\",\n                     breaks = seq(0, .8, by = .2)) +\n\n  theme_classic()\n\nprint(income_graph)"
  },
  {
    "objectID": "chapter2.html#page-27-standardized-scores",
    "href": "chapter2.html#page-27-standardized-scores",
    "title": "2  Review",
    "section": "2.3 Page 27 Standardized Scores",
    "text": "2.3 Page 27 Standardized Scores\n\nsample1_oz &lt;- c(40, 45, 50, 55, 60, 65, 70)\nz_sample1_oz &lt;- scale(sample1_oz, center = TRUE, scale = TRUE)\n\n\nsample2_grams &lt;- c(1100, 1150, 1200, 1400, 1700, 1725, 1775)\nz_sample2_grams &lt;- scale(sample2_grams, center = TRUE, scale = TRUE)\n\n\n\n\n\n\nsample1_oz\nsample2_grams\nz_sample1_oz\nz_sample2_grams\n\n\n\n\n40\n1100\n-1.3887301\n-1.1405606\n\n\n45\n1150\n-0.9258201\n-0.9706899\n\n\n50\n1200\n-0.4629100\n-0.8008191\n\n\n55\n1400\n0.0000000\n-0.1213362\n\n\n60\n1700\n0.4629100\n0.8978881\n\n\n65\n1725\n0.9258201\n0.9828235\n\n\n70\n1775\n1.3887301\n1.1526942"
  },
  {
    "objectID": "chapter2.html#page-30-correlation-covariance",
    "href": "chapter2.html#page-30-correlation-covariance",
    "title": "2  Review",
    "section": "2.4 Page 30 Correlation /Covariance",
    "text": "2.4 Page 30 Correlation /Covariance\n\nsample1_oz &lt;- c(40, 45, 50, 55, 60, 65, 70)\nsample1_length = c(31, 33, 37, 38, 42, 45, 48)\n\ncov_sample1 &lt;- cov(sample1_oz, sample1_length)\nprint(cov_sample1)\n\n[1] 66.66667\n\ncorrelation_sample1 &lt;- cor(sample1_oz, sample1_length)\nprint(correlation_sample1)\n\n[1] 0.9950372\n\n\nBut also note:\n\n# covariance is like correlation, but with the standard deviations included\n\ncorrelation_sample1 * sd(sample1_oz) * sd(sample1_length)\n\n[1] 66.66667\n\n# You get the same value as the covariance\nprint(cov_sample1)\n\n[1] 66.66667"
  },
  {
    "objectID": "chapter2.html#page-34-nations-data-inspection",
    "href": "chapter2.html#page-34-nations-data-inspection",
    "title": "2  Review",
    "section": "2.5 Page 34 Nations Data Inspection",
    "text": "2.5 Page 34 Nations Data Inspection\n\n2.5.1 Activate packages\n\nlibrary(usethis) # use_github_file() \nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\n\n\n\n2.5.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n2.5.2.1 Load data from your computer\n\nnations2018 &lt;- read_csv(\"nations2018.csv\") %&gt;% \n  clean_names()\n\n\n\n\n2.5.3 Inspect data\n\nnations2018 %&gt;% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   &lt;chr&gt; \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   &lt;dbl&gt; 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen &lt;dbl&gt; 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor &lt;dbl&gt; 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\nOr use\n\nnations2018 %&gt;% \n  view()  \n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n2.5.4 Descriptive with skim()\n\nlibrary(skimr)\n\nnations2018 %&gt;% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n8\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nnation\n0\n1\n5\n14\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpend\n0\n1\n19.79\n2.87\n14.1\n18.88\n19.80\n21.42\n23.4\n▂▁▅▇▅\n\n\neconopen\n0\n1\n59.05\n19.87\n27.1\n52.98\n61.75\n67.42\n87.4\n▅▁▇▂▅\n\n\nperlabor\n0\n1\n24.55\n16.72\n8.8\n14.90\n20.20\n28.02\n60.3\n▇▃▂▁▂\n\n\n\n\n\n\n\n2.5.5 Descriptives with describe()\nAlternatively, you could use the describe() command from the psych package as per the book. But, NEVER use library(psych) it will break the tidyverse. Instead use psyc:: before each psych package command.\n\n# psych package must be installed. But do not use library(psych)\n# Notice how psych creates a mean for the nation column - which makes no sense\nnations2018 %&gt;% \n  psych::describe()  \n\n         vars n  mean    sd median trimmed   mad  min  max range  skew kurtosis\nnation*     1 8  4.50  2.45   4.50    4.50  2.97  1.0  8.0   7.0  0.00    -1.65\nexpend      2 8 19.79  2.87  19.80   19.79  1.85 14.1 23.4   9.3 -0.60    -0.62\neconopen    3 8 59.05 19.87  61.75   59.05 12.75 27.1 87.4  60.3 -0.31    -1.29\nperlabor    4 8 24.55 16.72  20.20   24.55 11.71  8.8 60.3  51.5  1.04    -0.19\n           se\nnation*  0.87\nexpend   1.01\neconopen 7.02\nperlabor 5.91"
  },
  {
    "objectID": "chapter2.html#page-34-gss-t-test",
    "href": "chapter2.html#page-34-gss-t-test",
    "title": "2  Review",
    "section": "2.6 Page 34 GSS t-test",
    "text": "2.6 Page 34 GSS t-test\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim\n\n\n2.6.0.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n2.6.0.2 Load data from your computer\n\ngss2018 &lt;- read_csv(\"gss2018.csv\") %&gt;% \n  clean_names()\n\n\n\n2.6.0.3 Inspect data\n\ngss2018 %&gt;% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     &lt;chr&gt; \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        &lt;dbl&gt; 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     &lt;dbl&gt; 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     &lt;chr&gt; \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    &lt;dbl&gt; 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    &lt;dbl&gt; 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    &lt;dbl&gt; 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    &lt;dbl&gt; 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        &lt;dbl&gt; 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   &lt;dbl&gt; 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     &lt;dbl&gt; 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      &lt;dbl&gt; 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       &lt;chr&gt; \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij &lt;chr&gt; NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   &lt;dbl&gt; 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   &lt;dbl&gt; 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   &lt;dbl&gt; 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  &lt;dbl&gt; NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  &lt;dbl&gt; 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence &lt;dbl&gt; 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty &lt;dbl&gt; 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    &lt;dbl&gt; 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n2.6.0.4 skimr(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %&gt;% \n  select(pincome, female) %&gt;%\n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2315\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nfemale\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npincome\n0\n1\n9.42\n8.95\n0\n0\n9\n18\n26\n▇▂▂▅▂\n\n\n\n\n\n\n\n2.6.0.5 psyc::describe(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %&gt;% \n  select(pincome, female) %&gt;%\n  psych::describe()  \n\n        vars    n mean   sd median trimmed   mad min max range skew kurtosis\npincome    1 2315 9.42 8.95      9    8.85 13.34   0  26    26 0.21    -1.55\nfemale*    2 2315 1.45 0.50      1    1.44  0.00   1   2     1 0.20    -1.96\n          se\npincome 0.19\nfemale* 0.01\n\n\n\n\n2.6.0.6 Standard t.test\n\nt.test(gss2018$pincome ~ gss2018$female )\n\n\n    Welch Two Sample t-test\n\ndata:  gss2018$pincome by gss2018$female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n\n2.6.0.7 apaText t.test\n\nlibrary(apaText)\n\n# This code provides markdown text for Quarto documents\n\ngss2018 %&gt;%\n  mutate(female = as.factor(female)) %&gt;%\n  apa.ind.t.test(female, pincome, var.equal = FALSE)\n\n[1] \"$\\\\Delta M$ = 2.66, 95% CI[1.93, 3.39], *t*(2123.48) = 7.12, *p* &lt; .001\""
  },
  {
    "objectID": "chapter2.html#page-35-chapter-exercises",
    "href": "chapter2.html#page-35-chapter-exercises",
    "title": "2  Review",
    "section": "2.7 Page 35 Chapter Exercises",
    "text": "2.7 Page 35 Chapter Exercises\n\n2.7.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n2.7.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/American.csv\",\n                save_as = \"americans.csv\")\n\n\n\n2.7.3 Load data from your computer\n\namericans &lt;- read_csv(\"americans.csv\") %&gt;% \n  clean_names()\n\n\n\n2.7.4 Inspect data\n\namericans %&gt;% \n  glimpse()  \n\nRows: 2,797\nColumns: 4\n$ id       &lt;dbl&gt; 1030, 1059, 1076, 1079, 1084, 1092, 1102, 1110, 1115, 1117, 1…\n$ educate  &lt;dbl&gt; 14, 14, 10, 16, 14, 14, 12, 14, 16, 16, 14, 20, 12, 20, 14, 1…\n$ american &lt;dbl&gt; 2.00000, 14.00000, 20.00000, 10.00000, 12.50000, 26.66667, 4.…\n$ group    &lt;chr&gt; \"Not immigrant\", \"Not immigrant\", \"Not immigrant\", \"Not immig…\n\n\n\n\n2.7.5 Hints\n\nlibrary(tidyverse)\n\nglimpse(americans)\n\nRows: 2,797\nColumns: 4\n$ id       &lt;dbl&gt; 1030, 1059, 1076, 1079, 1084, 1092, 1102, 1110, 1115, 1117, 1…\n$ educate  &lt;dbl&gt; 14, 14, 10, 16, 14, 14, 12, 14, 16, 16, 14, 20, 12, 20, 14, 1…\n$ american &lt;dbl&gt; 2.00000, 14.00000, 20.00000, 10.00000, 12.50000, 26.66667, 4.…\n$ group    &lt;chr&gt; \"Not immigrant\", \"Not immigrant\", \"Not immigrant\", \"Not immig…\n\n\nNotice the column, group, is a chr column. We need to make it a factor.\n\namericans &lt;- americans %&gt;%\n  mutate(group = as_factor(group))\n\nNow skim() that column. Recall skim() is from the skimr package.\n\namericans %&gt;%\n  select(group) %&gt;%\n  skim()\n\nCompare the reults of the two commands below. Be sure to read the documentation for geom_jitter. Use ?geom_jitter in the console.\n\nggplot(americans, aes(x=educate, y = american)) + \n  geom_point()\n\n\nggplot(americans, aes(x=educate, y = american)) + \n  geom_jitter()"
  },
  {
    "objectID": "chapter3.html#required-packages",
    "href": "chapter3.html#required-packages",
    "title": "3  Simple Linear Regression Models",
    "section": "3.1 Required packages",
    "text": "3.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\napaTables\n\n\ntidymodels\n\n\nbroom\n\n\npsych\n\n\n\nREMINDER: Never use the command library(psych)."
  },
  {
    "objectID": "chapter3.html#page-40-graphing-introduction",
    "href": "chapter3.html#page-40-graphing-introduction",
    "title": "3  Simple Linear Regression Models",
    "section": "3.2 Page 40 Graphing Introduction",
    "text": "3.2 Page 40 Graphing Introduction\n\n3.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n3.2.3 Load data from your computer\n\nnations2018 &lt;- read_csv(\"nations2018.csv\") %&gt;% \n  clean_names()\n\n\n\n3.2.4 Inspect data\n\nnations2018 %&gt;% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   &lt;chr&gt; \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   &lt;dbl&gt; 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen &lt;dbl&gt; 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor &lt;dbl&gt; 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n3.2.4.1 Graph\n\nnations_plot &lt;- ggplot(data = nations2018,\n                        mapping = aes(x = perlabor,\n                                      y = expend)) +\n  geom_point(shape = 18) +\n  geom_text(mapping = aes(label = nation),\n            nudge_y = .4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(5, 65),\n                  ylim = c(14, 24)) +\n  scale_x_continuous(breaks = seq(5, 65, by = 10)) +\n  scale_y_continuous(breaks = seq(14, 24, by = 2)) +\n  labs(x = \"Percent labor union\",\n       y = \"Public expenditures\",\n       title = \"Public Expenditures vs Percent Labor Union\")\n\n  \nprint(nations_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapter3.html#pages-44-to-46-analysis-and-graphing",
    "href": "chapter3.html#pages-44-to-46-analysis-and-graphing",
    "title": "3  Simple Linear Regression Models",
    "section": "3.3 Pages 44 to 46 Analysis and Graphing",
    "text": "3.3 Pages 44 to 46 Analysis and Graphing\n\n3.3.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.3.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n\n3.3.3 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 &lt;- read_csv(\"statedata2018.csv\") %&gt;% \n  clean_names()\n\n\n3.3.3.1 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %&gt;% \n  select(sort(names(statedata2018))) %&gt;%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           &lt;dbl&gt; 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     &lt;dbl&gt; 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               &lt;dbl&gt; 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               &lt;dbl&gt; 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                &lt;dbl&gt; 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       &lt;dbl&gt; 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              &lt;dbl&gt; 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            &lt;chr&gt; \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              &lt;chr&gt; \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      &lt;dbl&gt; 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      &lt;dbl&gt; 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               &lt;dbl&gt; 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                &lt;dbl&gt; 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  &lt;dbl&gt; 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             &lt;dbl&gt; 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            &lt;dbl&gt; 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               &lt;dbl&gt; 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              &lt;dbl&gt; 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  &lt;dbl&gt; 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        &lt;dbl&gt; 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       &lt;dbl&gt; 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            &lt;dbl&gt; 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      &lt;dbl&gt; 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year &lt;dbl&gt; 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   &lt;dbl&gt; 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      &lt;dbl&gt; 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            &lt;dbl&gt; 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           &lt;dbl&gt; 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               &lt;dbl&gt; 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       &lt;dbl&gt; 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            &lt;dbl&gt; 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 &lt;dbl&gt; 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             &lt;dbl&gt; 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              &lt;dbl&gt; 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 &lt;dbl&gt; 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   &lt;dbl&gt; 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   &lt;dbl&gt; 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             &lt;dbl&gt; 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       &lt;dbl&gt; 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        &lt;dbl&gt; 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                &lt;dbl&gt; 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               &lt;dbl&gt; 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               &lt;dbl&gt; 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               &lt;dbl&gt; 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              &lt;dbl&gt; 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         &lt;dbl&gt; 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  &lt;dbl&gt; 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  &lt;dbl&gt; 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             &lt;dbl&gt; 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          &lt;dbl&gt; 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 &lt;dbl&gt; 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      &lt;dbl&gt; 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                &lt;dbl&gt; 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  &lt;dbl&gt; 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          &lt;dbl&gt; 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          &lt;dbl&gt; 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                &lt;dbl&gt; 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             &lt;dbl&gt; 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 &lt;dbl&gt; 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                &lt;dbl&gt; 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  &lt;dbl&gt; 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        &lt;dbl&gt; 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            &lt;dbl&gt; 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  &lt;dbl&gt; 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  &lt;dbl&gt; 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               &lt;dbl&gt; 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     &lt;dbl&gt; 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          &lt;dbl&gt; 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     &lt;dbl&gt; 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               &lt;dbl&gt; 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         &lt;dbl&gt; 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        &lt;dbl&gt; 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              &lt;dbl&gt; 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         &lt;dbl&gt; 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        &lt;dbl&gt; 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n\n3.3.4 Graph\n\nstate_plot &lt;- ggplot(data = statedata2018,\n                     mapping = aes(x = life_satis,\n                                   y = opioid_od_death_rate)) +\n  geom_point(shape = 1) +\n  geom_text(mapping = aes(label = state),\n            nudge_y = 1,\n            size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(49, 55),\n                  ylim = c(0, 50)) +\n  scale_x_continuous(breaks = seq(49, 55, by = 1)) +\n  scale_y_continuous(breaks = seq(10, 50, by = 10)) +\n  labs(x = \"Average Life Satisfaction\",\n       y = \"Opioid overdose deaths per 100,000\",\n       title = \"Opioid Deaths vs Life Satisfaction\") +\n  theme_light()\n\nprint(state_plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n3.3.5 Textbook Approach: Analysis\n\nlrm3_1 &lt;- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nsummary(lrm3_1)\n\n\nCall:\nlm(formula = opioid_od_death_rate ~ life_satis, data = statedata2018)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.688  -6.952  -1.511   3.408  28.118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  212.056     56.576   3.748 0.000479 ***\nlife_satis    -3.792      1.093  -3.468 0.001116 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.292 on 48 degrees of freedom\nMultiple R-squared:  0.2004,    Adjusted R-squared:  0.1837 \nF-statistic: 12.03 on 1 and 48 DF,  p-value: 0.001116\n\nconfint(lrm3_1)\n\n                2.5 %     97.5 %\n(Intercept) 98.302367 325.809877\nlife_satis  -5.989863  -1.593489\n\n\n\n\n3.3.6 Recommended Approach: Analysis\nThe tidy() command gives you p-values for each predictor. The glance() command gives you overall fit statistics.\n\nlibrary(tidymodels)\n\nlrm3_1 &lt;- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\n\ntidy(lrm3_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n212.056122\n56.57604\n3.748161\n0.0004785\n\n\nlife_satis\n-3.791676\n1.09328\n-3.468165\n0.0011161\n\n\n\n\n\n\nglance(lrm3_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.2003754\n0.1837166\n9.291875\n12.02817\n0.0011161\n1\n-181.3834\n368.7668\n374.5029\n4144.269\n48\n50\n\n\n\n\n\napa.reg.table() from apaTables package gives you confidence intervals and beta-weights. It’s one step and combines everything into one table.\n\nlrm3_1 &lt;- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(apaTables)\ntable1 &lt;- apa.reg.table(lrm3_1)\n\napa.save(\"table1.doc\", table1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]"
  },
  {
    "objectID": "chapter3.html#pages-49-to-50-predictedfitted-values",
    "href": "chapter3.html#pages-49-to-50-predictedfitted-values",
    "title": "3  Simple Linear Regression Models",
    "section": "3.4 Pages 49 to 50 Predicted/Fitted Values",
    "text": "3.4 Pages 49 to 50 Predicted/Fitted Values\nWhen you look at the graph with life_satis on the x-axis and opioid_od_death_rate on the y-axis you see a regression line. The points that fall on this line are predicted scores for opioid_od_death_rate based on life_satis. Alternatively, we might call these fitted values for opioid_od_death_rate based on life_satis. We can obtained a predicted score (i.e., fitted score) for each x-axis value using augment() command fromm the broom package. When you inspect the output below only pay attention to the opioid_od_death_rate, life_satis, and .fitted columns. The column pioid_od_death_rate is the measured opiod overdose death rate, the column life_satis is the measured life satisfaction, the column .fitted is the predicted opiod overdose death rate for a given life_satis value.\n\nlrm3_1 &lt;- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(broom)\nlrm3_1 %&gt;%\n  augment()\n\n# A tibble: 50 × 8\n   opioid_od_death_rate life_satis .fitted  .resid   .hat .sigma   .cooksd\n                  &lt;dbl&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1                  9         51.4    17.1  -8.08  0.0214   9.31 0.00844  \n 2                 13.9       52.1    14.6  -0.676 0.0217   9.39 0.0000598\n 3                 13.5       51.4    17.2  -3.72  0.0217   9.37 0.00182  \n 4                  6.5       52.3    13.6  -7.07  0.0252   9.33 0.00768  \n 5                  5.3       51.5    16.8 -11.5   0.0208   9.24 0.0167   \n 6                 10         52.9    11.6  -1.56  0.0381   9.39 0.000579 \n 7                 27.7       50.5    20.6   7.08  0.0415   9.33 0.0131   \n 8                 27.8       52.7    12.1  15.7   0.0340   9.10 0.0521   \n 9                 16.3       51.1    18.4  -2.13  0.0262   9.38 0.000726 \n10                  9.7       51.2    17.9  -8.23  0.0240   9.31 0.00988  \n# ℹ 40 more rows\n# ℹ 1 more variable: .std.resid &lt;dbl&gt;\n\n\n\n3.4.1 Fitted values and percentiles\nSee previous section for loading the data.\nWe want to predict opioid overdose at the 25th, 50th, and 75th percentiles for life satisfaction. So we obtain the life satisfaction values corresponding to these percentiles below.\n\nstatedata2018 %&gt;%\n  select(life_satis) %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlife_satis\n0\n1\n51.73\n1.21\n49.07\n51.08\n51.63\n52.33\n55.63\n▃▇▇▂▁\n\n\n\n\n\nFrom the above skim() output we extract the percentile information and put it in a table that’s easy to follow below. We can see the life_satis value for each percentile in this table.\n\n\n\nPercentile\nlife_satis value\n\n\n\n\n25th\n51.1\n\n\n50th\n51.6\n\n\n75th\n52.3\n\n\n\n\n\n3.4.2 Calculate fitted values\n\nlrm3_1 &lt;- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\n\n# we need to use the EXACT name from the original data set\nlife_satis &lt;- c(51.1, 51.6, 52.3)\n\nfit_for_values = data.frame(life_satis)\n\npredict(lrm3_1, fit_for_values)\n\n       1        2        3 \n18.30148 16.40564 13.75146 \n\n\nThe values above are the predicted values for opioid_od_death_rate. We put everything in the table below for clarity. The predicted opioid_od_death_rate value provides a corresponding point on the regression line. That is, all (life_satis value, predicted opioid_od_death_rate value) points fall on the regression line.\n\n\n\n\n\n\n\n\nPercentile life_satis\nlife_satis value\npredicted opioid_od_death_rate value\n\n\n\n\n25th\n51.1\n18.30148\n\n\n50th\n51.6\n16.40564\n\n\n75th\n52.3\n13.75146\n\n\n\nRecall the regression formulas:\n\\[\n\\hat{y} = b_0 + b_1X\n\\]\nIn the context of our variables: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nRecall the regression output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this output we get the regression formula:\nThis formula \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nBecomes: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = 212.06 + -3.79(life\\_satis)\n\\]\nTherfore for our three points:\n\\[\n\\begin{aligned}\n18.30148 &= 212.06 + -3.79(51.1)\\\\\n16.40564 &= 212.06 + -3.79(51.6)\\\\\n13.75146 &= 212.06 + -3.79(52.3)\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html#page-62-chapter-exercises",
    "href": "chapter3.html#page-62-chapter-exercises",
    "title": "3  Simple Linear Regression Models",
    "section": "3.5 Page 62 Chapter Exercises",
    "text": "3.5 Page 62 Chapter Exercises\n\n3.5.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.5.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/HighSchool.csv\",\n                save_as = \"highschool.csv\")\n\n\n\n3.5.3 Load data from your computer\n\nhighschool &lt;- read_csv(\"highschool.csv\") %&gt;% \n  clean_names()\n\n\n\n3.5.4 Inspect data\n\nhighschool %&gt;% \n  glimpse()  \n\nRows: 178\nColumns: 6\n$ row                  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ id_number            &lt;dbl&gt; 2583454, 758606, 6918338, 757890, 4584594, 858964…\n$ sports_participation &lt;dbl&gt; 0.00, 1.10, 0.00, 0.00, 1.10, 1.10, 0.69, 0.00, 1…\n$ academic_clubs       &lt;dbl&gt; 1.61, 0.69, 1.10, 0.00, 0.69, 0.00, 0.69, 1.61, 1…\n$ alcohol_use          &lt;dbl&gt; 1.39, 0.69, 0.69, 0.69, 0.00, 1.10, 1.95, 0.69, 1…\n$ gpa                  &lt;dbl&gt; 3.25, 4.00, 4.00, 2.88, 4.00, 2.25, 2.12, 3.62, 3…\n\n\n\n\n3.5.5 Helpful Commands for Exercise\n\n# horizontal line\ngeom_hline(yintercept = 22)\n\n\n# Challenge help. Use the .resid column in reg_diagnostics\nlibrary(broom)\nreg_diagnostics &lt;- lrm_for_your_data %&gt;% augment()"
  },
  {
    "objectID": "chapter4.html#required-packages",
    "href": "chapter4.html#required-packages",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.1 Required packages",
    "text": "4.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\napaTables\n\n\nbroom\n\n\ncorrr\n\n\ntidymodels\n\n\nremotes\n\n\nrelaimpo\n\n\npsych\n\n\n\nREMINDER:\nNever use the command library(psych). Instead use psych:: before each command.\nNever use the command library(relaimpo). Instead use relaimpo:: before each command.\nThe following GitHub packages must be installed:\n\n\n\nRequired GitHub Packages\n\n\n\n\ndstanley4/fastInteraction\n\n\n\nAfter the remotes package is installed, it can be used to install a package from GitHub:\n\nremotes::install_github(\"dstanley4/fastInteraction\")"
  },
  {
    "objectID": "chapter4.html#page-66-correlations",
    "href": "chapter4.html#page-66-correlations",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.2 Page 66 Correlations",
    "text": "4.2 Page 66 Correlations\n\n4.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n4.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n4.2.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 &lt;- read_csv(\"statedata2018.csv\") %&gt;% \n  clean_names()\n\n\n\n\n4.2.3 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %&gt;% \n  select(sort(names(statedata2018))) %&gt;%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           &lt;dbl&gt; 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     &lt;dbl&gt; 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               &lt;dbl&gt; 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               &lt;dbl&gt; 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                &lt;dbl&gt; 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       &lt;dbl&gt; 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              &lt;dbl&gt; 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            &lt;chr&gt; \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              &lt;chr&gt; \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      &lt;dbl&gt; 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      &lt;dbl&gt; 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               &lt;dbl&gt; 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                &lt;dbl&gt; 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  &lt;dbl&gt; 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             &lt;dbl&gt; 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            &lt;dbl&gt; 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               &lt;dbl&gt; 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              &lt;dbl&gt; 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  &lt;dbl&gt; 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        &lt;dbl&gt; 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       &lt;dbl&gt; 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            &lt;dbl&gt; 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      &lt;dbl&gt; 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year &lt;dbl&gt; 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   &lt;dbl&gt; 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      &lt;dbl&gt; 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            &lt;dbl&gt; 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           &lt;dbl&gt; 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               &lt;dbl&gt; 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       &lt;dbl&gt; 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            &lt;dbl&gt; 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 &lt;dbl&gt; 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             &lt;dbl&gt; 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              &lt;dbl&gt; 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 &lt;dbl&gt; 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   &lt;dbl&gt; 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   &lt;dbl&gt; 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             &lt;dbl&gt; 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       &lt;dbl&gt; 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        &lt;dbl&gt; 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                &lt;dbl&gt; 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               &lt;dbl&gt; 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               &lt;dbl&gt; 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               &lt;dbl&gt; 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              &lt;dbl&gt; 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         &lt;dbl&gt; 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  &lt;dbl&gt; 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  &lt;dbl&gt; 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             &lt;dbl&gt; 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          &lt;dbl&gt; 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 &lt;dbl&gt; 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      &lt;dbl&gt; 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                &lt;dbl&gt; 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  &lt;dbl&gt; 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          &lt;dbl&gt; 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          &lt;dbl&gt; 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                &lt;dbl&gt; 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             &lt;dbl&gt; 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 &lt;dbl&gt; 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                &lt;dbl&gt; 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  &lt;dbl&gt; 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        &lt;dbl&gt; 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            &lt;dbl&gt; 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  &lt;dbl&gt; 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  &lt;dbl&gt; 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               &lt;dbl&gt; 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     &lt;dbl&gt; 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          &lt;dbl&gt; 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     &lt;dbl&gt; 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               &lt;dbl&gt; 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         &lt;dbl&gt; 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        &lt;dbl&gt; 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              &lt;dbl&gt; 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         &lt;dbl&gt; 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        &lt;dbl&gt; 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n4.2.4 Select focal variables\n\nfocal_data &lt;- statedata2018 %&gt;%\n  select(violent_crime_rate, per_child_poverty, med_hh_income)\n\n\n\n4.2.5 Correlation options\n\n4.2.5.1 psych package\n\nfocal_data %&gt;% \n  psych::corr.test()\n\nCall:psych::corr.test(x = .)\nCorrelation matrix \n                   violent_crime_rate per_child_poverty med_hh_income\nviolent_crime_rate               1.00              0.49         -0.21\nper_child_poverty                0.49              1.00         -0.76\nmed_hh_income                   -0.21             -0.76          1.00\nSample Size \n[1] 50\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n                   violent_crime_rate per_child_poverty med_hh_income\nviolent_crime_rate               0.00                 0          0.15\nper_child_poverty                0.00                 0          0.00\nmed_hh_income                    0.15                 0          0.00\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n\n4.2.5.2 apaTables package\n\nlibrary(apaTables)\n\nfocal_data %&gt;% \n  apa.cor.table()\n\n\n\nDescriptive Statistics and Correlations\n \n\n  Variable              M        SD      1           2           \n  1. violent_crime_rate 346.81   128.82                          \n                                                                 \n                                                                 \n                                                                 \n  2. per_child_poverty  16.84    4.75    .49**                   \n                                         [.25, .68]              \n                                         p &lt; .001                \n                                                                 \n  3. med_hh_income      60252.12 9879.50 -.21        -.76**      \n                                         [-.46, .07] [-.85, -.61]\n                                         p = .147    p &lt; .001    \n                                                                 \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\n * indicates p &lt; .05. ** indicates p &lt; .01.\n \n\n\n\n\n4.2.5.3 corrr package\nWe use correlate() to get the correlations, shave() to remove upper diagonal, and fashion() to make it nice:\n\nlibrary(corrr)\n\nfocal_data %&gt;% \n  correlate() %&gt;%\n  shave() %&gt;%\n  fashion()\n\n                term violent_crime_rate per_child_poverty med_hh_income\n1 violent_crime_rate                                                   \n2  per_child_poverty                .49                                \n3      med_hh_income               -.21              -.76              \n\n\nBut more importantly the corrr package has network_plot() to visual relations among variables. Here we only plot relations where the magnitude of the correlation is greater than .20:\n\nfocal_data %&gt;% \n  correlate() %&gt;%\n  network_plot(min_cor = .2,\n               colors = c(\"red\", \"green\"), \n               legend = \"full\")"
  },
  {
    "objectID": "chapter4.html#page-67-one-predictor",
    "href": "chapter4.html#page-67-one-predictor",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.3 Page 67 One Predictor",
    "text": "4.3 Page 67 One Predictor\n\nlm4_1 &lt;- lm(violent_crime_rate ~ per_child_poverty,\n            data = focal_data)\n\ntidy(lm4_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n122.24404\n59.556226\n2.052582\n0.0455868\n\n\nper_child_poverty\n13.33503\n3.406162\n3.914974\n0.0002850\n\n\n\n\n\n\nglance(lm4_1)\n\n\nknitr::kable(glance(lm4_1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.2420297\n0.2262387\n113.3141\n15.32702\n0.000285\n1\n-306.4346\n618.8692\n624.6052\n616324.6\n48\n50\n\n\n\n\n\n\nlibrary(apaTables)\ntable1 &lt;- apa.reg.table(lm4_1)\n\napa.save(\"table1.doc\", table1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n122.24*\n[2.50, 241.99]\n\n\n\n\n\n\n\n\nper_child_poverty\n13.34**\n[6.49, 20.18]\n0.49\n[0.24, 0.74]\n.24**\n[.06, .42]\n.49**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .242**\n\n\n\n\n\n\n\n\n\n\n95% CI[.06,.42]"
  },
  {
    "objectID": "chapter4.html#page-68-two-predictors",
    "href": "chapter4.html#page-68-two-predictors",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.4 Page 68 Two Predictors",
    "text": "4.4 Page 68 Two Predictors\n\nlm4_2 &lt;- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\ntidy(lm4_2)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-310.5658212\n217.9130166\n-1.425183\n0.1607118\n\n\nper_child_poverty\n21.1797652\n5.0375180\n4.204405\n0.0001164\n\n\nmed_hh_income\n0.0049908\n0.0024233\n2.059517\n0.0450080\n\n\n\n\n\n\nglance(lm4_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.3047719\n0.2751878\n109.6714\n10.30186\n0.000195\n2\n-304.2745\n616.5489\n624.197\n565307.3\n47\n50\n\n\n\n\n\n\nlibrary(apaTables)\ntable1 &lt;- apa.reg.table(lm4_2)\n\napa.save(\"table1.doc\", table1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n-310.57\n[-748.95, 127.82]\n\n\n\n\n\n\n\n\nper_child_poverty\n21.18**\n[11.05, 31.31]\n0.78\n[0.41, 1.16]\n.26**\n[.06, .47]\n.49**\n\n\n\nmed_hh_income\n0.00*\n[0.00, 0.01]\n0.38\n[0.01, 0.76]\n.06*\n[-.05, .18]\n-.21\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .305**\n\n\n\n\n\n\n\n\n\n\n95% CI[.09,.47]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1 Two Predictors: Predicted Scores\nTwo help us interpret the data we will make a graph. But to do so we need to know the range for per_child_poverty. We find it ranges from 9 percent to 28 percent from the skim() output below.\n\nfocal_data %&gt;%\n  select(per_child_poverty) %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nper_child_poverty\n0\n1\n16.84\n4.75\n9\n13\n16.5\n20\n28\n▅▇▇▅▂\n\n\n\n\n\nNow we want to know the mean of med_hh_income below. We find it is 60252.\n\nfocal_data %&gt;%\n  select(med_hh_income) %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nmed_hh_income\n0\n1\n60252.12\n9879.5\n43567\n53133.5\n59162.5\n67687.25\n81868\n▅▇▇▃▃\n\n\n\n\n\nNow we want to HOLD med_hh_income constant at 60252 and then see how violent_crime_rate changes with per_child_poverty. We create a dataset where this is the case:\n\npredict4values &lt;- data.frame(per_child_poverty = seq(9, 28), \n                             med_hh_income = 60252)\n\nprint(predict4values)\n\n   per_child_poverty med_hh_income\n1                  9         60252\n2                 10         60252\n3                 11         60252\n4                 12         60252\n5                 13         60252\n6                 14         60252\n7                 15         60252\n8                 16         60252\n9                 17         60252\n10                18         60252\n11                19         60252\n12                20         60252\n13                21         60252\n14                22         60252\n15                23         60252\n16                24         60252\n17                25         60252\n18                26         60252\n19                27         60252\n20                28         60252\n\n\nWe use our regression model to generate predicted scores:\n\n# Create predicted scores use the regression weights created in lm4_2\npredicted_violent_crime_rate &lt;- stats::predict(lm4_2, \n                                        newdata = predict4values)\n\n# Put the predicted scores back into our data set of possible values\npredict4values &lt;- predict4values %&gt;%\n  mutate(predicted_violent_crime_rate = predicted_violent_crime_rate)\n  \n\nprint(predict4values)  \n\n   per_child_poverty med_hh_income predicted_violent_crime_rate\n1                  9         60252                     180.7560\n2                 10         60252                     201.9358\n3                 11         60252                     223.1156\n4                 12         60252                     244.2953\n5                 13         60252                     265.4751\n6                 14         60252                     286.6549\n7                 15         60252                     307.8346\n8                 16         60252                     329.0144\n9                 17         60252                     350.1942\n10                18         60252                     371.3739\n11                19         60252                     392.5537\n12                20         60252                     413.7335\n13                21         60252                     434.9132\n14                22         60252                     456.0930\n15                23         60252                     477.2728\n16                24         60252                     498.4525\n17                25         60252                     519.6323\n18                26         60252                     540.8121\n19                27         60252                     561.9918\n20                28         60252                     583.1716\n\n\nNow graph it:\n\nprediction_graph &lt;- ggplot(data = predict4values,\n                           mapping = aes(x = per_child_poverty,\n                                         y = predicted_violent_crime_rate)) +\n  geom_line(linewidth = 2) +\n  coord_cartesian(xlim = c(9, 28), ylim = c(100, 700)) +\n  scale_x_continuous(breaks = seq(10, 25, by = 5)) +\n  scale_y_continuous(breaks = seq(100, 700, by = 100)) +\n  theme_light() +\n  labs(x = \"Percent Child Poverty\",\n       y = \"Predicted Violent Crime Rate (yhat)\")\n\nprint(prediction_graph)"
  },
  {
    "objectID": "chapter4.html#page-71-3d-plot",
    "href": "chapter4.html#page-71-3d-plot",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.5 Page 71 3D plot",
    "text": "4.5 Page 71 3D plot\nWhen you have two predictors you don’t have a regression line - you have a regression surface. The code below creates a surface plot that you can interact with/rotate/etc. Note that even though we put med_hh_income in the moderator position in the function there is no moderation here. Be sure to click and drag to rotate the graph.\n\nlibrary(fastInteraction)\n\nsurface_plot &lt;- fast.plot(lm4_2,\n                          criterion = violent_crime_rate,\n                          predictor = med_hh_income,\n                          moderator = per_child_poverty)\n\n\nsurface_plot"
  },
  {
    "objectID": "chapter4.html#page-72-understanding-b-weights",
    "href": "chapter4.html#page-72-understanding-b-weights",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.6 Page 72 Understanding b-weights",
    "text": "4.6 Page 72 Understanding b-weights\nSee the original regression below. Notice the b-weight (the unstandardized regression weight) for per_child_poverty is 21.1797652. We are going to try to recreate this value in another way to make it clear what it means.\n\n# Original regression\nlm4_2 &lt;- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\ntidy(lm4_2)\n\n\n# Original regression\nlm4_2 &lt;- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\nknitr::kable(tidy(lm4_2))\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-310.5658212\n217.9130166\n-1.425183\n0.1607118\n\n\nper_child_poverty\n21.1797652\n5.0375180\n4.204405\n0.0001164\n\n\nmed_hh_income\n0.0049908\n0.0024233\n2.059517\n0.0450080\n\n\n\n\n\nFirst, we create a residualized version of violent_crime_rate. That is, a version of violent_crime_rate has teh effect of med_hh_income removed from it. We use the residual() command. This is the same as getting the value from the .resid column after using the augment() command.\n\nlibrary(tidymodels)\n\nlm_violent_crime_rate &lt;- lm(violent_crime_rate ~ med_hh_income, data = focal_data)\nviolent_crime_rate_residual &lt;- lm_violent_crime_rate$residuals\n\nSecond, we create a residualized version of per_child_poverty. That is, a version of per_child_poverty has teh effect of med_hh_income removed from it.\n\nlm_per_child_poverty &lt;- lm(per_child_poverty ~  med_hh_income, data = focal_data)\nper_child_poverty_residual &lt;- lm_per_child_poverty$residuals\n\nTo get the b-weight or the original regression (lm4_2) for per_child_poverty we conduct a bivariate regression with these two residuals.\n\nlm_bweight_demo &lt;- lm(violent_crime_rate_residual ~ per_child_poverty_residual)\n\ntidy(lm_bweight_demo)\n\n\nlm_bweight_demo &lt;- lm(violent_crime_rate_residual ~ per_child_poverty_residual)\n\nknitr::kable(tidy(lm_bweight_demo))\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.00000\n15.347466\n0.000000\n1.00e+00\n\n\nper_child_poverty_residual\n21.17977\n4.984768\n4.248897\n9.83e-05\n\n\n\n\n\nNotice that we get 21.1797652 as the b-weight for per_child_poverty_residual here. This is the same as the b-weight for per_child_poverty (no residual suffix) in the original regression lm4_2. The diagram beloow illustrate that b-weight in a multiple regression are weights based on residualized verions of a predictor and the criterion."
  },
  {
    "objectID": "chapter4.html#page-76-b--vs-beta-weights",
    "href": "chapter4.html#page-76-b--vs-beta-weights",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.7 Page 76 b- vs beta-weights",
    "text": "4.7 Page 76 b- vs beta-weights\nThe regression weights that we have looked at so far - are usually referred to as b-weights or unstandardized regression coefficients. The are obtained when we analyze the data in it’s original form. That is, each variable has. a different mean and standard deviation. Sometimes, however, we want to be able to compare the magnitude of regression weights. To do so, we need to set the mean of each variable (predictor or criterion) to zero prior to analysis. As well, we need to set the standard deviation of each variable prior to analysis. That is, we need to standrdized (M=0, SD =1) each variable prior to analysis. The weights we obtain when we do this are referred to as beta-weights or standardized regression cofficients. A better name, not used, would be regression weights for standardized data. We can obtain this weights by analyzing the data as described or by transforming b-weights using the formula in the textboook. Standardized regression weights (beta-weights) can be obtained using the apa.reg.table() command in the apaTables package.\n\nlm4_3 &lt;- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\ntable1 = apa.reg.table(lm4_3, table.number = 1)\n\napa.save(\"table1.doc\", table1)"
  },
  {
    "objectID": "chapter4.html#page-77-relative-importance",
    "href": "chapter4.html#page-77-relative-importance",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.8 Page 77 Relative importance",
    "text": "4.8 Page 77 Relative importance\n\n# It's always a good idea to get the set of focal variables \n# in a separate data set.  Once you have missing data,\n# this approach is critical.\nfocal_data &lt;- statedata2018 %&gt;%\n  select(violent_crime_rate, \n         per_child_poverty, \n         med_hh_income, \n         unemploy_rate,\n         percent_uninsured)\n\nlm4_4 &lt;- lm(violent_crime_rate ~ per_child_poverty + \n              med_hh_income + unemploy_rate + percent_uninsured,\n            data = focal_data)\n\n\ntidy(lm4_4)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-398.4877332\n219.0412666\n-1.819236\n0.0755328\n\n\nper_child_poverty\n15.4179235\n5.9126931\n2.607597\n0.0123269\n\n\nmed_hh_income\n0.0046877\n0.0025789\n1.817686\n0.0757736\n\n\nunemploy_rate\n27.6326568\n23.1503710\n1.193616\n0.2388829\n\n\npercent_uninsured\n10.3132584\n4.7224539\n2.183877\n0.0342242\n\n\n\n\n\nThen you can obtain relative importance information:\n\nx_compare &lt;- relaimpo::calc.relimp(lm4_4, \n                         type = c(\"lmg\",\n                                  \"first\", \n                                  \"last\", \n                                  \"betasq\", \n                                  \"pratt\"))\nprint(x_compare)\n\nResponse variable: violent_crime_rate \nTotal response variance: 16594.39 \nAnalysis based on 50 observations \n\n4 Regressors: \nper_child_poverty med_hh_income unemploy_rate percent_uninsured \nProportion of variance explained by model: 39.44%\nMetrics are not normalized (rela=FALSE). \n\nRelative importance metrics: \n\n                         lmg       last      first     betasq       pratt\nper_child_poverty 0.15529495 0.09150503 0.24202970 0.32354317  0.27983398\nmed_hh_income     0.03895609 0.04446327 0.04329193 0.12924768 -0.07480229\nunemploy_rate     0.10605740 0.01917314 0.20001828 0.02941550  0.07670488\npercent_uninsured 0.09410481 0.06418303 0.15848929 0.08010657  0.11267667\n\nAverage coefficients for different model sizes: \n\n                            1X          2Xs          3Xs          4Xs\nper_child_poverty 13.335033251 1.390782e+01 14.375405724 15.417923450\nmed_hh_income     -0.002712995 7.508337e-04  0.003029558  0.004687669\nunemploy_rate     72.055866278 5.874625e+01 46.216351078 27.632656766\npercent_uninsured 14.506471774 1.140058e+01 10.243657931 10.313258363\n\n\nThen make the plots:\n\nplot(x_compare)"
  },
  {
    "objectID": "chapter4.html#page-79-predicted-means",
    "href": "chapter4.html#page-79-predicted-means",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.9 Page 79 Predicted Means",
    "text": "4.9 Page 79 Predicted Means\nRecall the regression:\n\nfocal_data &lt;- statedata2018 %&gt;%\n  select(violent_crime_rate, \n         per_child_poverty, \n         med_hh_income)\n\nlm4_3 &lt;- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\n\ntidy(lm4_3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-310.5658212\n217.9130166\n-1.425183\n0.1607118\n\n\nper_child_poverty\n21.1797652\n5.0375180\n4.204405\n0.0001164\n\n\nmed_hh_income\n0.0049908\n0.0024233\n2.059517\n0.0450080\n\n\n\n\n\nWe need the percentiles for per_child_poverty and med_hh_income.\n\nfocal_data %&gt;%\n  select(per_child_poverty, med_hh_income) %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nper_child_poverty\n0\n1\n16.84\n4.75\n9\n13.0\n16.5\n20.00\n28\n▅▇▇▅▂\n\n\nmed_hh_income\n0\n1\n60252.12\n9879.50\n43567\n53133.5\n59162.5\n67687.25\n81868\n▅▇▇▃▃\n\n\n\n\n\nFor per_child_poverty: 25th percentile is 13, 75th percentile is 20. We want to look at these with reference to the 50th percentile of med_hh_income which is 59162.\n\npredict4values &lt;- data.frame(per_child_poverty = c(13, 20),\n                             med_hh_income = 59162.)\n\nprint(predict4values)\n\n  per_child_poverty med_hh_income\n1                13         59162\n2                20         59162\n\nstats::predict(lm4_3, newdata = predict4values)\n\n       1        2 \n260.0352 408.2935 \n\n\nFor med_hh_income: 25th percentile is 53134, 75th percentile is 67687 We want to look at these with reference to the 50th percentile of per_child_poverty which is 16.5\n\npredict4values &lt;- data.frame(per_child_poverty = 16.5,\n                             med_hh_income = c(53134, 67687))\n\nprint(predict4values)\n\n  per_child_poverty med_hh_income\n1              16.5         53134\n2              16.5         67687\n\nstats::predict(lm4_3, newdata = predict4values)\n\n       1        2 \n304.0800 376.7107"
  },
  {
    "objectID": "chapter4.html#page-86-chapter-exercises",
    "href": "chapter4.html#page-86-chapter-exercises",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.10 Page 86 Chapter Exercises",
    "text": "4.10 Page 86 Chapter Exercises\n\n4.10.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n4.10.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/TeenBirths.csv\",\n                save_as = \"teenbirths.csv\")\n\n\n\n4.10.3 Load data from your computer\n\nteenbirths &lt;- read_csv(\"teenbirths.csv\") %&gt;% \n  clean_names()\n\n\n\n4.10.4 Inspect data\n\nteenbirths %&gt;% \n  glimpse()  \n\nRows: 2,946\nColumns: 7\n$ state             &lt;chr&gt; \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama…\n$ county            &lt;chr&gt; \"Autauga\", \"Baldwin\", \"Barbour\", \"Bibb\", \"Blount\", \"…\n$ teen_birth_rate   &lt;dbl&gt; 51, 50, 74, 59, 51, 90, 66, 58, 70, 62, 61, 53, 46, …\n$ per_uninsured     &lt;dbl&gt; 14, 16, 19, 18, 18, 21, 18, 16, 19, 18, 19, 18, 17, …\n$ per_hsgrads       &lt;dbl&gt; 85, 73, 64, 67, 75, 58, 76, 72, 61, 67, 73, 68, 74, …\n$ per_child_poverty &lt;dbl&gt; 18, 20, 36, 29, 24, 40, 40, 32, 44, 33, 32, 30, 34, …\n$ per_singleparent  &lt;dbl&gt; 30, 29, 52, 35, 25, 64, 50, 39, 46, 26, 30, 37, 40, …\n\n\n\nteenbirths &lt;- teenbirths %&gt;% \n  mutate(state = as_factor(state)) %&gt;%\n  mutate(county = as_factor(county))\n\n\nteenbirths %&gt;% \n  glimpse()  \n\nRows: 2,946\nColumns: 7\n$ state             &lt;fct&gt; Alabama, Alabama, Alabama, Alabama, Alabama, Alabama…\n$ county            &lt;fct&gt; Autauga, Baldwin, Barbour, Bibb, Blount, Bullock, Bu…\n$ teen_birth_rate   &lt;dbl&gt; 51, 50, 74, 59, 51, 90, 66, 58, 70, 62, 61, 53, 46, …\n$ per_uninsured     &lt;dbl&gt; 14, 16, 19, 18, 18, 21, 18, 16, 19, 18, 19, 18, 17, …\n$ per_hsgrads       &lt;dbl&gt; 85, 73, 64, 67, 75, 58, 76, 72, 61, 67, 73, 68, 74, …\n$ per_child_poverty &lt;dbl&gt; 18, 20, 36, 29, 24, 40, 40, 32, 44, 33, 32, 30, 34, …\n$ per_singleparent  &lt;dbl&gt; 30, 29, 52, 35, 25, 64, 50, 39, 46, 26, 30, 37, 40, …\n\n\n\nteenbirths %&gt;% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2946\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n50\nTex: 238, Geo: 158, Vir: 128, Ken: 119\n\n\ncounty\n0\n1\nFALSE\n1744\nWas: 31, Jef: 26, Fra: 24, Jac: 23\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nteen_birth_rate\n0\n1\n47.44\n20.56\n4\n31\n46\n61\n127\n▃▇▅▂▁\n\n\nper_uninsured\n0\n1\n18.30\n5.75\n3\n14\n18\n22\n43\n▂▇▆▁▁\n\n\nper_hsgrads\n0\n1\n82.08\n9.67\n14\n77\n83\n89\n100\n▁▁▁▆▇\n\n\nper_child_poverty\n0\n1\n24.44\n9.10\n3\n18\n24\n30\n61\n▂▇▅▁▁\n\n\nper_singleparent\n0\n1\n31.14\n9.85\n4\n25\n30\n36\n76\n▁▇▅▁▁"
  },
  {
    "objectID": "chapter5.html#required-packages",
    "href": "chapter5.html#required-packages",
    "title": "5  Goodness of Fit",
    "section": "5.1 Required packages",
    "text": "5.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\ntidymodels"
  },
  {
    "objectID": "chapter5.html#page-89-fit-introduction",
    "href": "chapter5.html#page-89-fit-introduction",
    "title": "5  Goodness of Fit",
    "section": "5.2 Page 89 Fit Introduction",
    "text": "5.2 Page 89 Fit Introduction\n\n5.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n5.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n5.2.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 &lt;- read_csv(\"statedata2018.csv\") %&gt;% \n  clean_names()\n\n\n\n\n5.2.3 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %&gt;% \n  select(sort(names(statedata2018))) %&gt;%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           &lt;dbl&gt; 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     &lt;dbl&gt; 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               &lt;dbl&gt; 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               &lt;dbl&gt; 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                &lt;dbl&gt; 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       &lt;dbl&gt; 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              &lt;dbl&gt; 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            &lt;chr&gt; \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              &lt;chr&gt; \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      &lt;dbl&gt; 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      &lt;dbl&gt; 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               &lt;dbl&gt; 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                &lt;dbl&gt; 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  &lt;dbl&gt; 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             &lt;dbl&gt; 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            &lt;dbl&gt; 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               &lt;dbl&gt; 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              &lt;dbl&gt; 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  &lt;dbl&gt; 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        &lt;dbl&gt; 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       &lt;dbl&gt; 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            &lt;dbl&gt; 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      &lt;dbl&gt; 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year &lt;dbl&gt; 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   &lt;dbl&gt; 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      &lt;dbl&gt; 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            &lt;dbl&gt; 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           &lt;dbl&gt; 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               &lt;dbl&gt; 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       &lt;dbl&gt; 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            &lt;dbl&gt; 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 &lt;dbl&gt; 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             &lt;dbl&gt; 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              &lt;dbl&gt; 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 &lt;dbl&gt; 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   &lt;dbl&gt; 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   &lt;dbl&gt; 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             &lt;dbl&gt; 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       &lt;dbl&gt; 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        &lt;dbl&gt; 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                &lt;dbl&gt; 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               &lt;dbl&gt; 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               &lt;dbl&gt; 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               &lt;dbl&gt; 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              &lt;dbl&gt; 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         &lt;dbl&gt; 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  &lt;dbl&gt; 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  &lt;dbl&gt; 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             &lt;dbl&gt; 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          &lt;dbl&gt; 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 &lt;dbl&gt; 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      &lt;dbl&gt; 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                &lt;dbl&gt; 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  &lt;dbl&gt; 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          &lt;dbl&gt; 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          &lt;dbl&gt; 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                &lt;dbl&gt; 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             &lt;dbl&gt; 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 &lt;dbl&gt; 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                &lt;dbl&gt; 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  &lt;dbl&gt; 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        &lt;dbl&gt; 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            &lt;dbl&gt; 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  &lt;dbl&gt; 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  &lt;dbl&gt; 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               &lt;dbl&gt; 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     &lt;dbl&gt; 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          &lt;dbl&gt; 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     &lt;dbl&gt; 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               &lt;dbl&gt; 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         &lt;dbl&gt; 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        &lt;dbl&gt; 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              &lt;dbl&gt; 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         &lt;dbl&gt; 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        &lt;dbl&gt; 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n5.2.4 Select focal variables\n\nfocal_data &lt;- statedata2018 %&gt;%\n  select(violent_crime_rate, per_child_poverty)\n\n\n\n5.2.5 Conduct Regression\n\nlm5_1 &lt;- lm(violent_crime_rate ~ per_child_poverty,\n            data = focal_data)\n\n\n\n5.2.6 Approach 1: Regression fit\n\nsummary(lm5_1)\n\n\nCall:\nlm(formula = violent_crime_rate ~ per_child_poverty, data = focal_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-217.12  -59.48   -9.75   43.76  340.20 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        122.244     59.556   2.053 0.045587 *  \nper_child_poverty   13.335      3.406   3.915 0.000285 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 113.3 on 48 degrees of freedom\nMultiple R-squared:  0.242, Adjusted R-squared:  0.2262 \nF-statistic: 15.33 on 1 and 48 DF,  p-value: 0.000285\n\nanova(lm5_1)\n\nAnalysis of Variance Table\n\nResponse: violent_crime_rate\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nper_child_poverty  1 196800  196800  15.327 0.000285 ***\nResiduals         48 616325   12840                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.2.7 Approach 2: Regression fit (better)\n\nlibrary(tidymodels)\ntidy(lm5_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n122.24404\n59.556226\n2.052582\n0.0455868\n\n\nper_child_poverty\n13.33503\n3.406162\n3.914974\n0.0002850\n\n\n\n\n\n\nglance(lm5_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.2420297\n0.2262387\n113.3141\n15.32702\n0.000285\n1\n-306.4346\n618.8692\n624.6052\n616324.6\n48\n50\n\n\n\n\n\n\nanova(lm5_1)\n\nAnalysis of Variance Table\n\nResponse: violent_crime_rate\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nper_child_poverty  1 196800  196800  15.327 0.000285 ***\nResiduals         48 616325   12840                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "chapter5.html#page-96-single-value-prediction-interval",
    "href": "chapter5.html#page-96-single-value-prediction-interval",
    "title": "5  Goodness of Fit",
    "section": "5.3 Page 96 Single Value Prediction Interval",
    "text": "5.3 Page 96 Single Value Prediction Interval\n\n5.3.1 Load Data\n\nfocal_data &lt;- statedata2018 %&gt;%\n  select(violent_crime_rate, per_child_poverty)\n\n\n\n5.3.2 Conduct Regression\n\nlm5_2 &lt;- lm(violent_crime_rate ~ per_child_poverty, \n            data = focal_data)\n\n\n# Specify values for which you want predictions\npredict4value = data.frame(per_child_poverty = 16.5)\nprint(predict4value)\n\n  per_child_poverty\n1              16.5\n\n# Create prediction interval\nstats::predict(lm5_2, \n        newdata = predict4value,\n        interval = \"prediction\")\n\n       fit      lwr      upr\n1 342.2721 112.1599 572.3843\n\n\n\n\n5.3.3 Graph Single Value PI\nThe plot below illustrates the prediction interval.\n\nggplot(data = focal_data,\n       mapping = aes(x = per_child_poverty,\n                     y = violent_crime_rate)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  annotate(geom = \"segment\",\n           x = 16.5, xend = 16.5,\n           y = 112.1599, yend = 572.3843,\n           linewidth = 1.5,\n           color = \"red\") +\n  scale_x_continuous(breaks = seq(9, 30, by = 1)) +\n  scale_y_continuous(breaks = seq(100, 700, by = 100))\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nRecall that we have a sample. The sample is a subset of the population of values. The red line in the plot indicates a range that will capture 95% of population values when x = 16.5."
  },
  {
    "objectID": "chapter5.html#pages-97-98-x-axis-prediction-interval",
    "href": "chapter5.html#pages-97-98-x-axis-prediction-interval",
    "title": "5  Goodness of Fit",
    "section": "5.4 Pages 97-98 x-axis Prediction Interval",
    "text": "5.4 Pages 97-98 x-axis Prediction Interval\nCreate a prediction inteval for all values on the x-axis.\n\n5.4.1 Recall Regression\n\nlm5_2 &lt;- lm(violent_crime_rate ~ per_child_poverty, \n            data = focal_data)"
  },
  {
    "objectID": "chapter5.html#determine-x-axis-range",
    "href": "chapter5.html#determine-x-axis-range",
    "title": "5  Goodness of Fit",
    "section": "5.5 Determine x-axis range",
    "text": "5.5 Determine x-axis range\n\nlibrary(skimr)\n\nfocal_data %&gt;%\n  select(per_child_poverty) %&gt;%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nper_child_poverty\n0\n1\n16.84\n4.75\n9\n13\n16.5\n20\n28\n▅▇▇▅▂\n\n\n\n\n\nWe find x-axis values (per_child_poverty) range from 9 to 28."
  },
  {
    "objectID": "chapter5.html#specify-values-for-prediction-interval",
    "href": "chapter5.html#specify-values-for-prediction-interval",
    "title": "5  Goodness of Fit",
    "section": "5.6 Specify values for prediction interval",
    "text": "5.6 Specify values for prediction interval\n\n# Specify values for which you want predictions\npredict4value = data.frame(per_child_poverty = seq(9, 28))\nprint(predict4value)\n\n   per_child_poverty\n1                  9\n2                 10\n3                 11\n4                 12\n5                 13\n6                 14\n7                 15\n8                 16\n9                 17\n10                18\n11                19\n12                20\n13                21\n14                22\n15                23\n16                24\n17                25\n18                26\n19                27\n20                28\n\n# Create prediction interval\nprediction_interval_values&lt;- stats::predict(lm5_2, \n                                            newdata = predict4value,\n                                            interval = \"prediction\")\n\n# make this a data set not an array of numbers\nprediction_interval_values &lt;- as_tibble(prediction_interval_values)\n\nprint(prediction_interval_values)\n\n# A tibble: 20 × 3\n     fit    lwr   upr\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1  242.   5.98  479.\n 2  256.  20.8   490.\n 3  269.  35.4   502.\n 4  282.  49.8   515.\n 5  296.  64.0   527.\n 6  309.  78.0   540.\n 7  322.  91.8   553.\n 8  336. 105.    566.\n 9  349. 119.    579.\n10  362. 132.    593.\n11  376. 145.    606.\n12  389. 158.    620.\n13  402. 170.    634.\n14  416. 183.    648.\n15  429. 195.    663.\n16  442. 207.    678.\n17  456. 219.    692.\n18  469. 230.    707.\n19  482. 242.    723.\n20  496. 253.    738."
  },
  {
    "objectID": "chapter5.html#add-x-values-to-prediction_interval_values",
    "href": "chapter5.html#add-x-values-to-prediction_interval_values",
    "title": "5  Goodness of Fit",
    "section": "5.7 Add x-values to prediction_interval_values",
    "text": "5.7 Add x-values to prediction_interval_values\n\nprediction_interval_values &lt;- prediction_interval_values %&gt;%\n  mutate(per_child_poverty = seq(9, 28))\n\nprint(prediction_interval_values)\n\n# A tibble: 20 × 4\n     fit    lwr   upr per_child_poverty\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;             &lt;int&gt;\n 1  242.   5.98  479.                 9\n 2  256.  20.8   490.                10\n 3  269.  35.4   502.                11\n 4  282.  49.8   515.                12\n 5  296.  64.0   527.                13\n 6  309.  78.0   540.                14\n 7  322.  91.8   553.                15\n 8  336. 105.    566.                16\n 9  349. 119.    579.                17\n10  362. 132.    593.                18\n11  376. 145.    606.                19\n12  389. 158.    620.                20\n13  402. 170.    634.                21\n14  416. 183.    648.                22\n15  429. 195.    663.                23\n16  442. 207.    678.                24\n17  456. 219.    692.                25\n18  469. 230.    707.                26\n19  482. 242.    723.                27\n20  496. 253.    738.                28\n\n\nWe will use the above data set to plot the prediction interval.\n\n5.7.1 Graph x-axis PI\nThe plot below illustrates the prediction interval using the geom_ribbon() command and the prediction_intevals_values data. The alpha argument geom_ribbon() indicates that the red fill will be 20% nontransparent or 80% transparent.\nAlso notice this is pretty fancy ggplot() code. We are actually using two data sets in our plot. For most of the plot commands the focal_data is used. However, for the geom_ribbon() command we indicate that the prediction_intevals_values data set should be used.\n\nggplot(data = focal_data,\n       mapping = aes(x = per_child_poverty,\n                     y = violent_crime_rate)) +\n  geom_ribbon(data = prediction_interval_values,\n              mapping = aes(x = per_child_poverty,\n                            y = fit,\n                            ymin = lwr,\n                            ymax = upr),\n              fill = \"red\",\n              alpha= .2) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  scale_x_continuous(breaks = seq(9, 30, by = 1)) +\n  scale_y_continuous(breaks = seq(100, 700, by = 100))\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapter5.html#pages-98-99",
    "href": "chapter5.html#pages-98-99",
    "title": "5  Goodness of Fit",
    "section": "5.8 Pages 98-99",
    "text": "5.8 Pages 98-99\n\n5.8.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n5.8.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GPA.csv\",\n                save_as = \"gpa.csv\")\n\n\n5.8.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nlibrary(tidyverse)\n\ngpa &lt;- read_csv(\"gpa.csv\") %&gt;% \n  clean_names()\n\n\n\n\n5.8.3 Inspect data\n\ngpa %&gt;% glimpse()\n\nRows: 20\nColumns: 5\n$ gpa            &lt;dbl&gt; 1.97, 2.74, 2.19, 2.60, 2.98, 1.65, 1.89, 2.38, 2.66, 1…\n$ hs_math        &lt;dbl&gt; 2.50, 4.00, 3.18, 3.78, 3.58, 1.68, 1.87, 3.93, 3.29, 1…\n$ hs_english     &lt;dbl&gt; 2.83, 3.77, 2.77, 2.41, 3.68, 2.34, 2.84, 2.72, 3.40, 3…\n$ sat_math       &lt;dbl&gt; 321, 718, 358, 403, 640, 237, 270, 418, 443, 359, 669, …\n$ sat_read_write &lt;dbl&gt; 247, 436, 578, 447, 563, 342, 472, 356, 327, 385, 664, …\n\n\n\nlibrary(skimr)\ngpa %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n20\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngpa\n0\n1\n2.59\n0.62\n1.65\n2.01\n2.62\n3.08\n3.90\n▇▃▇▅▂\n\n\nhs_math\n0\n1\n3.05\n0.85\n1.67\n2.34\n3.37\n3.72\n4.00\n▅▁▁▅▇\n\n\nhs_english\n0\n1\n3.21\n0.48\n2.34\n2.82\n3.26\n3.66\n3.94\n▂▇▂▅▇\n\n\nsat_math\n0\n1\n511.60\n166.20\n237.00\n392.00\n486.00\n647.00\n791.00\n▃▇▁▇▃\n\n\nsat_read_write\n0\n1\n478.25\n132.62\n247.00\n362.00\n459.50\n585.00\n704.00\n▂▇▇▃▅\n\n\n\n\n\n\n\n5.8.4 Rescale data\n\ngpa %&gt;%\n  mutate(sat_math = sat_math/100) %&gt;%\n  mutate(sat_read_write = sat_read_write/100)\n\n# A tibble: 20 × 5\n     gpa hs_math hs_english sat_math sat_read_write\n   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;          &lt;dbl&gt;\n 1  1.97    2.5        2.83     3.21           2.47\n 2  2.74    4          3.77     7.18           4.36\n 3  2.19    3.18       2.77     3.58           5.78\n 4  2.6     3.78       2.41     4.03           4.47\n 5  2.98    3.58       3.68     6.4            5.63\n 6  1.65    1.68       2.34     2.37           3.42\n 7  1.89    1.87       2.84     2.7            4.72\n 8  2.38    3.93       2.72     4.18           3.56\n 9  2.66    3.29       3.4      4.43           3.27\n10  1.96    1.74       3.66     3.59           3.85\n11  3.14    3.41       3.57     6.69           6.64\n12  1.96    2.97       2.8      4.09           5.18\n13  2.2     1.67       3.1      5.82           3.64\n14  3.9     3.34       3.69     7.5            6.32\n15  2.02    1.74       3.4      4.51           4.35\n16  3.61    3.7        3.94     6.45           7.04\n17  3.07    3.4        3.13     7.91           3.41\n18  2.63    3.79       3.52     5.21           4.83\n19  3.11    3.62       2.9      5.94           6.65\n20  3.2     3.89       3.72     6.53           6.06\n\n\n\ngpa %&gt;% skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n20\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\ngpa\n0\n1\n2.59\n0.62\n1.65\n2.01\n2.62\n3.08\n3.90\n▇▃▇▅▂\n\n\nhs_math\n0\n1\n3.05\n0.85\n1.67\n2.34\n3.37\n3.72\n4.00\n▅▁▁▅▇\n\n\nhs_english\n0\n1\n3.21\n0.48\n2.34\n2.82\n3.26\n3.66\n3.94\n▂▇▂▅▇\n\n\nsat_math\n0\n1\n511.60\n166.20\n237.00\n392.00\n486.00\n647.00\n791.00\n▃▇▁▇▃\n\n\nsat_read_write\n0\n1\n478.25\n132.62\n247.00\n362.00\n459.50\n585.00\n704.00\n▂▇▇▃▅\n\n\n\n\n\n\n\n5.8.5 Textbook Approach: Regression\n\nlm5_3 &lt;- lm(gpa ~ sat_math + sat_read_write + hs_math, data = gpa)\nsummary(lm5_3)\n\n\nCall:\nlm(formula = gpa ~ sat_math + sat_read_write + hs_math, data = gpa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.44589 -0.12849 -0.01274  0.10086  0.53291 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    0.2982757  0.2649782   1.126 0.276911    \nsat_math       0.0021849  0.0004553   4.799 0.000197 ***\nsat_read_write 0.0013123  0.0005252   2.499 0.023738 *  \nhs_math        0.1798702  0.0876786   2.051 0.056964 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2621 on 16 degrees of freedom\nMultiple R-squared:  0.8504,    Adjusted R-squared:  0.8223 \nF-statistic: 30.31 on 3 and 16 DF,  p-value: 7.816e-07\n\nconfint(lm5_3)\n\n                       2.5 %      97.5 %\n(Intercept)    -0.2634528725 0.860004340\nsat_math        0.0012196317 0.003150112\nsat_read_write  0.0001989283 0.002425724\nhs_math        -0.0060000644 0.365740550\n\nanova(lm5_3)\n\nAnalysis of Variance Table\n\nResponse: gpa\n               Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nsat_math        1 5.3015  5.3015 77.1663 1.614e-07 ***\nsat_read_write  1 0.6559  0.6559  9.5468  0.007029 ** \nhs_math         1 0.2891  0.2891  4.2085  0.056964 .  \nResiduals      16 1.0992  0.0687                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n5.8.6 Recommended Approach: Regression\n\nlm5_3 &lt;- lm(gpa ~ sat_math + sat_read_write + hs_math, data = gpa)\n\n\nlibrary(tidymodels)\ntidy(lm5_3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.2982757\n0.2649782\n1.125662\n0.2769110\n\n\nsat_math\n0.0021849\n0.0004553\n4.798517\n0.0001969\n\n\nsat_read_write\n0.0013123\n0.0005252\n2.498664\n0.0237379\n\n\nhs_math\n0.1798702\n0.0876786\n2.051473\n0.0569637\n\n\n\n\n\n\nglance(lm5_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.8503577\n0.8222998\n0.2621122\n30.30722\n8e-07\n3\n0.6323138\n8.735373\n13.71403\n1.099245\n16\n20\n\n\n\n\n\n\nlibrary(apaTables)\ntable1 &lt;- apa.reg.table(lm5_3)\n\napa.save(\"table1.doc\", table1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n0.30\n[-0.26, 0.86]\n\n\n\n\n\n\n\n\nsat_math\n0.00**\n[0.00, 0.00]\n0.58\n[0.33, 0.84]\n.22**\n[.01, .42]\n.85**\n\n\n\nsat_read_write\n0.00*\n[0.00, 0.00]\n0.28\n[0.04, 0.52]\n.06*\n[-.03, .15]\n.65**\n\n\n\nhs_math\n0.18\n[-0.01, 0.37]\n0.25\n[-0.01, 0.50]\n.04\n[-.03, .11]\n.69**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .850**\n\n\n\n\n\n\n\n\n\n\n95% CI[.60,.90]"
  },
  {
    "objectID": "chapter5.html#pages-100-101-comparing-b-weights",
    "href": "chapter5.html#pages-100-101-comparing-b-weights",
    "title": "5  Goodness of Fit",
    "section": "5.9 Pages 100-101 Comparing b-weights",
    "text": "5.9 Pages 100-101 Comparing b-weights\n\nlibrary(car)\n\nlinearHypothesis(lm5_3, \"sat_math = sat_read_write\")\n\nLinear hypothesis test\n\nHypothesis:\nsat_math - sat_read_write = 0\n\nModel 1: restricted model\nModel 2: gpa ~ sat_math + sat_read_write + hs_math\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     17 1.1837                           \n2     16 1.0993  1  0.084409 1.2286 0.2841"
  },
  {
    "objectID": "chapter5.html#chapter-exercises",
    "href": "chapter5.html#chapter-exercises",
    "title": "5  Goodness of Fit",
    "section": "5.10 Chapter Exercises",
    "text": "5.10 Chapter Exercises\n\n5.10.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n5.10.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/QBPassers2019.csv\",\n                save_as = \"qbpassers2019.csv\")\n\n\n5.10.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nqbpassers &lt;- read_csv(\"qbpassers2019.csv\") %&gt;% \n  clean_names()\n\n\n\n\n5.10.3 Inspect data\n\nqbpassers %&gt;% \n  glimpse()  \n\nRows: 50\nColumns: 15\n$ name          &lt;chr&gt; \"Aaron Rodgers\", \"Russell Wilson\", \"Drew Brees\", \"Tony R…\n$ firstyear     &lt;dbl&gt; 2005, 2012, 2001, 2004, 2000, 2016, 2012, 1985, 1998, 20…\n$ lastyear      &lt;dbl&gt; NA, NA, NA, 2017, NA, NA, NA, 1999, 2015, NA, NA, NA, 20…\n$ games         &lt;dbl&gt; 181, 128, 275, 150, 285, 64, 93, 169, 266, 228, 189, 218…\n$ attempts      &lt;dbl&gt; 6061, 3777, 10161, 4335, 9988, 2071, 3146, 4149, 9380, 7…\n$ completions   &lt;dbl&gt; 3913, 2436, 6867, 2829, 6377, 1363, 2104, 2667, 6125, 49…\n$ compperc      &lt;dbl&gt; 64.6, 64.5, 67.6, 65.3, 63.8, 65.8, 66.9, 64.3, 65.3, 64…\n$ avercomp      &lt;dbl&gt; 12.0, 12.2, 11.3, 12.1, 11.7, 11.6, 11.5, 12.4, 11.7, 12…\n$ yards         &lt;dbl&gt; 46946, 29734, 77416, 34183, 74571, 15778, 24107, 33124, …\n$ interceptions &lt;dbl&gt; 84, 68, 237, 117, 179, 36, 71, 107, 251, 198, 147, 191, …\n$ interatt      &lt;dbl&gt; 1.39, 1.80, 2.33, 2.70, 1.79, 1.74, 2.26, 2.58, 2.68, 2.…\n$ touchdowns    &lt;dbl&gt; 364, 227, 547, 248, 541, 97, 155, 232, 539, 397, 321, 36…\n$ tdsatt        &lt;dbl&gt; 6.01, 6.01, 5.38, 5.72, 5.42, 4.68, 4.93, 5.59, 5.75, 5.…\n$ longest_pass  &lt;dbl&gt; 93, 80, 98, 85, 99, 90, 81, 97, 86, 84, 93, 97, 85, 73, …\n$ rating        &lt;dbl&gt; 102.4, 101.2, 98.4, 97.1, 97.0, 97.0, 96.8, 96.8, 96.5, …"
  },
  {
    "objectID": "chapter6.html#required-packages",
    "href": "chapter6.html#required-packages",
    "title": "6  Comparing Models",
    "section": "6.1 Required packages",
    "text": "6.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\ntidymodels"
  },
  {
    "objectID": "chapter6.html#page-107-create-models-to-compare",
    "href": "chapter6.html#page-107-create-models-to-compare",
    "title": "6  Comparing Models",
    "section": "6.2 Page 107 Create Models to Compare",
    "text": "6.2 Page 107 Create Models to Compare\n\n6.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor) # clean_names() \n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(tidymodels) # clean_names() \n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n✔ broom        1.0.4     ✔ rsample      1.1.1\n✔ dials        1.2.0     ✔ tune         1.1.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.0     ✔ yardstick    1.2.0\n✔ recipes      1.0.6     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\n\n\n\n6.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n\n6.2.3 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 &lt;- read_csv(\"statedata2018.csv\") %&gt;% \n  clean_names()\n\n\n\n6.2.4 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %&gt;% \n  select(sort(names(statedata2018))) %&gt;%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           &lt;dbl&gt; 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     &lt;dbl&gt; 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               &lt;dbl&gt; 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               &lt;dbl&gt; 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                &lt;dbl&gt; 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       &lt;dbl&gt; 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              &lt;dbl&gt; 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            &lt;chr&gt; \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              &lt;chr&gt; \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      &lt;dbl&gt; 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      &lt;dbl&gt; 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               &lt;dbl&gt; 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                &lt;dbl&gt; 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  &lt;dbl&gt; 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             &lt;dbl&gt; 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            &lt;dbl&gt; 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               &lt;dbl&gt; 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              &lt;dbl&gt; 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  &lt;dbl&gt; 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        &lt;dbl&gt; 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       &lt;dbl&gt; 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            &lt;dbl&gt; 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      &lt;dbl&gt; 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year &lt;dbl&gt; 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   &lt;dbl&gt; 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      &lt;dbl&gt; 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            &lt;dbl&gt; 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           &lt;dbl&gt; 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               &lt;dbl&gt; 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       &lt;dbl&gt; 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            &lt;dbl&gt; 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 &lt;dbl&gt; 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             &lt;dbl&gt; 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              &lt;dbl&gt; 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 &lt;dbl&gt; 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   &lt;dbl&gt; 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   &lt;dbl&gt; 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             &lt;dbl&gt; 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       &lt;dbl&gt; 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        &lt;dbl&gt; 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                &lt;dbl&gt; 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               &lt;dbl&gt; 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               &lt;dbl&gt; 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               &lt;dbl&gt; 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              &lt;dbl&gt; 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         &lt;dbl&gt; 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  &lt;dbl&gt; 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  &lt;dbl&gt; 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             &lt;dbl&gt; 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          &lt;dbl&gt; 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 &lt;dbl&gt; 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      &lt;dbl&gt; 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                &lt;dbl&gt; 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  &lt;dbl&gt; 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          &lt;dbl&gt; 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          &lt;dbl&gt; 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                &lt;dbl&gt; 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             &lt;dbl&gt; 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 &lt;dbl&gt; 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                &lt;dbl&gt; 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  &lt;dbl&gt; 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        &lt;dbl&gt; 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            &lt;dbl&gt; 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  &lt;dbl&gt; 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  &lt;dbl&gt; 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               &lt;dbl&gt; 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      &lt;chr&gt; \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     &lt;dbl&gt; 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          &lt;dbl&gt; 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     &lt;dbl&gt; 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               &lt;dbl&gt; 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         &lt;dbl&gt; 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        &lt;dbl&gt; 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              &lt;dbl&gt; 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         &lt;dbl&gt; 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        &lt;dbl&gt; 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n6.2.5 Both regression models\n\n# Block 1: Nested Model\nlm6_1  &lt;- lm(opioid_od_death_rate ~ life_satis + per_age0_18,\n             data = statedata2018)\n\n# Block 2: Full Model\nlm6_2  &lt;- lm(opioid_od_death_rate ~ life_satis + per_age0_18 + per_white + unemploy_rate,\n             data = statedata2018)\n\n\n\n6.2.6 Nested Model Output (Block 1)\n\ntidy(lm6_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n190.120029\n50.8221088\n3.740892\n0.0004981\n\n\nlife_satis\n-2.402546\n1.0468095\n-2.295113\n0.0262366\n\n\nper_age0_18\n-2.104995\n0.5766046\n-3.650673\n0.0006555\n\n\n\n\n\n\nglance(lm6_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.3770269\n0.3505174\n8.288322\n14.22233\n1.48e-05\n2\n-175.1424\n358.2848\n365.9329\n3228.725\n47\n50\n\n\n\n\n\n\nlibrary(apaTables)\ntable6_1 &lt;- apa.reg.table(lm6_1)\nprint(table6_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n190.12**\n[87.88, 292.36]\n\n\n\n\n\n\n\n\nlife_satis\n-2.40*\n[-4.51, -0.30]\n-0.28\n[-0.53, -0.04]\n.07*\n[-.04, .18]\n-.45**\n\n\n\nper_age0_18\n-2.10**\n[-3.26, -0.95]\n-0.45\n[-0.70, -0.20]\n.18**\n[.00, .35]\n-.55**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .377**\n\n\n\n\n\n\n\n\n\n\n95% CI[.15,.53]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.2.7 Full Model Output (Block 2)\n\ntidy(lm6_2)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n138.8430482\n57.0212812\n2.434934\n0.0189225\n\n\nlife_satis\n-2.0129097\n1.1544416\n-1.743622\n0.0880540\n\n\nper_age0_18\n-1.9641032\n0.5608326\n-3.502120\n0.0010549\n\n\nper_white\n0.2030293\n0.0735816\n2.759241\n0.0083483\n\n\nunemploy_rate\n3.6962343\n1.5833080\n2.334501\n0.0240916\n\n\n\n\n\n\nglance(lm6_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.5083352\n0.4646317\n7.525046\n11.63144\n1.4e-06\n4\n-169.2248\n350.4495\n361.9217\n2548.184\n45\n50\n\n\n\n\n\n\nlibrary(apaTables)\ntable6_1 &lt;- apa.reg.table(lm6_2)\nprint(table6_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n138.84*\n[24.00, 253.69]\n\n\n\n\n\n\n\n\nlife_satis\n-2.01\n[-4.34, 0.31]\n-0.24\n[-0.51, 0.04]\n.03\n[-.04, .10]\n-.45**\n\n\n\nper_age0_18\n-1.96**\n[-3.09, -0.83]\n-0.42\n[-0.66, -0.18]\n.13**\n[-.01, .27]\n-.55**\n\n\n\nper_white\n0.20**\n[0.05, 0.35]\n0.31\n[0.08, 0.54]\n.08**\n[-.03, .19]\n.25\n\n\n\nunemploy_rate\n3.70*\n[0.51, 6.89]\n0.29\n[0.04, 0.54]\n.06*\n[-.04, .15]\n.31*\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .508**\n\n\n\n\n\n\n\n\n\n\n95% CI[.25,.62]"
  },
  {
    "objectID": "chapter6.html#page-110-comparing-the-models",
    "href": "chapter6.html#page-110-comparing-the-models",
    "title": "6  Comparing Models",
    "section": "6.3 Page 110 Comparing the Models",
    "text": "6.3 Page 110 Comparing the Models\n\n6.3.1 Textbook Approach: Comparison\n\nanova(lm6_1, lm6_2)\n\nAnalysis of Variance Table\n\nModel 1: opioid_od_death_rate ~ life_satis + per_age0_18\nModel 2: opioid_od_death_rate ~ life_satis + per_age0_18 + per_white + \n    unemploy_rate\n  Res.Df    RSS Df Sum of Sq     F   Pr(&gt;F)   \n1     47 3228.7                               \n2     45 2548.2  2    680.54 6.009 0.004864 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n6.3.2 Recommended Approach: Comparison\n\nlibrary(apaTables)\ntable_compare &lt;- apa.reg.table(lm6_1, lm6_2)\nprint(table_compare)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\ndifference\n\n\n\n\nModel 1\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\n190.12**\n[87.88, 292.36]\n\n\n\n\n\n\n\n\n\nlife_satis\n-2.40*\n[-4.51, -0.30]\n-0.28\n[-0.53, -0.04]\n.07*\n[-.04, .18]\n-.45**\n\n\n\n\nper_age0_18\n-2.10**\n[-3.26, -0.95]\n-0.45\n[-0.70, -0.20]\n.18**\n[.00, .35]\n-.55**\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .377**\n\n\n\n\n\n\n\n\n\n\n\n95% CI[.15,.53]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 2\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\n138.84*\n[24.00, 253.69]\n\n\n\n\n\n\n\n\n\nlife_satis\n-2.01\n[-4.34, 0.31]\n-0.24\n[-0.51, 0.04]\n.03\n[-.04, .10]\n-.45**\n\n\n\n\nper_age0_18\n-1.96**\n[-3.09, -0.83]\n-0.42\n[-0.66, -0.18]\n.13**\n[-.01, .27]\n-.55**\n\n\n\n\nper_white\n0.20**\n[0.05, 0.35]\n0.31\n[0.08, 0.54]\n.08**\n[-.03, .19]\n.25\n\n\n\n\nunemploy_rate\n3.70*\n[0.51, 6.89]\n0.29\n[0.04, 0.54]\n.06*\n[-.04, .15]\n.31*\n\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .508**\n\\(\\Delta R^2\\) = .131**\n\n\n\n\n\n\n\n\n\n\n95% CI[.25,.62]\n95% CI[-.01, .27]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe two comparisons (Textbook and Recommended) have the same math “under the hood” but the reporting is different. You could use the F-value and p-value from the Textbook Approach along with the CI’s from the Recommended Approach."
  },
  {
    "objectID": "chapter6.html#page-112-aic-and-bic",
    "href": "chapter6.html#page-112-aic-and-bic",
    "title": "6  Comparing Models",
    "section": "6.4 Page 112 AIC and BIC",
    "text": "6.4 Page 112 AIC and BIC\n\n6.4.1 Nested Model (Block 1)\n\nglance(lm6_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.3770269\n0.3505174\n8.288322\n14.22233\n1.48e-05\n2\n-175.1424\n358.2848\n365.9329\n3228.725\n47\n50\n\n\n\n\n\n\n\n6.4.2 Full Model (Block 1)\n\nglance(lm6_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.5083352\n0.4646317\n7.525046\n11.63144\n1.4e-06\n4\n-169.2248\n350.4495\n361.9217\n2548.184\n45\n50"
  },
  {
    "objectID": "chapter6.html#chapter-exercises",
    "href": "chapter6.html#chapter-exercises",
    "title": "6  Comparing Models",
    "section": "6.5 Chapter Exercises",
    "text": "6.5 Chapter Exercises\n\n6.5.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n6.5.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/QBPassers2019.csv\",\n                save_as = \"qbpassers2019.csv\")\n\n\n6.5.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nqbpassers &lt;- read_csv(\"qbpassers2019.csv\") %&gt;% \n  clean_names()\n\n\n\n\n6.5.3 Inspect data\n\nqbpassers %&gt;% \n  glimpse()  \n\nRows: 50\nColumns: 15\n$ name          &lt;chr&gt; \"Aaron Rodgers\", \"Russell Wilson\", \"Drew Brees\", \"Tony R…\n$ firstyear     &lt;dbl&gt; 2005, 2012, 2001, 2004, 2000, 2016, 2012, 1985, 1998, 20…\n$ lastyear      &lt;dbl&gt; NA, NA, NA, 2017, NA, NA, NA, 1999, 2015, NA, NA, NA, 20…\n$ games         &lt;dbl&gt; 181, 128, 275, 150, 285, 64, 93, 169, 266, 228, 189, 218…\n$ attempts      &lt;dbl&gt; 6061, 3777, 10161, 4335, 9988, 2071, 3146, 4149, 9380, 7…\n$ completions   &lt;dbl&gt; 3913, 2436, 6867, 2829, 6377, 1363, 2104, 2667, 6125, 49…\n$ compperc      &lt;dbl&gt; 64.6, 64.5, 67.6, 65.3, 63.8, 65.8, 66.9, 64.3, 65.3, 64…\n$ avercomp      &lt;dbl&gt; 12.0, 12.2, 11.3, 12.1, 11.7, 11.6, 11.5, 12.4, 11.7, 12…\n$ yards         &lt;dbl&gt; 46946, 29734, 77416, 34183, 74571, 15778, 24107, 33124, …\n$ interceptions &lt;dbl&gt; 84, 68, 237, 117, 179, 36, 71, 107, 251, 198, 147, 191, …\n$ interatt      &lt;dbl&gt; 1.39, 1.80, 2.33, 2.70, 1.79, 1.74, 2.26, 2.58, 2.68, 2.…\n$ touchdowns    &lt;dbl&gt; 364, 227, 547, 248, 541, 97, 155, 232, 539, 397, 321, 36…\n$ tdsatt        &lt;dbl&gt; 6.01, 6.01, 5.38, 5.72, 5.42, 4.68, 4.93, 5.59, 5.75, 5.…\n$ longest_pass  &lt;dbl&gt; 93, 80, 98, 85, 99, 90, 81, 97, 86, 84, 93, 97, 85, 73, …\n$ rating        &lt;dbl&gt; 102.4, 101.2, 98.4, 97.1, 97.0, 97.0, 96.8, 96.8, 96.5, …"
  },
  {
    "objectID": "chapter7.html#required-packages",
    "href": "chapter7.html#required-packages",
    "title": "7  Indicator Variables",
    "section": "7.1 Required packages",
    "text": "7.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\ntidymodels"
  },
  {
    "objectID": "chapter7.html#page-118-single-categorical-predictor",
    "href": "chapter7.html#page-118-single-categorical-predictor",
    "title": "7  Indicator Variables",
    "section": "7.2 Page 118 Single Categorical Predictor",
    "text": "7.2 Page 118 Single Categorical Predictor\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv()\nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\nlibrary(tidymodels) #tidy() glance()\n\n\n7.2.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n7.2.2 Load data from your computer\n\ngss2018 &lt;- read_csv(\"gss2018.csv\") %&gt;% \n  clean_names()\n\n\n\n7.2.3 Inspect data\n\ngss2018 %&gt;% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     &lt;chr&gt; \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        &lt;dbl&gt; 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     &lt;dbl&gt; 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     &lt;chr&gt; \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    &lt;dbl&gt; 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    &lt;dbl&gt; 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    &lt;dbl&gt; 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    &lt;dbl&gt; 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        &lt;dbl&gt; 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   &lt;dbl&gt; 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     &lt;dbl&gt; 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      &lt;dbl&gt; 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       &lt;chr&gt; \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij &lt;chr&gt; NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   &lt;dbl&gt; 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   &lt;dbl&gt; 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   &lt;dbl&gt; 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  &lt;dbl&gt; NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  &lt;dbl&gt; 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence &lt;dbl&gt; 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty &lt;dbl&gt; 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    &lt;dbl&gt; 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\nNotice female is a chr variable.\n\n\n7.2.4 Making female a factor (don’t skip this step)\nWe need to make female a factor variable. Then indicate that female is the reference. That is female = 0 and male = 1.\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(female = as_factor(female)) %&gt;%\n  mutate(female = relevel(female, ref = \"female\"))\n\nSee how female is now a factor:\n\ngss2018 %&gt;% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     &lt;fct&gt; male, female, male, female, male, female, female, male, fem…\n$ age        &lt;dbl&gt; 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     &lt;dbl&gt; 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     &lt;chr&gt; \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    &lt;dbl&gt; 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    &lt;dbl&gt; 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    &lt;dbl&gt; 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    &lt;dbl&gt; 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        &lt;dbl&gt; 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   &lt;dbl&gt; 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     &lt;dbl&gt; 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      &lt;dbl&gt; 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       &lt;chr&gt; \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij &lt;chr&gt; NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   &lt;dbl&gt; 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   &lt;dbl&gt; 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   &lt;dbl&gt; 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  &lt;dbl&gt; NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  &lt;dbl&gt; 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence &lt;dbl&gt; 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty &lt;dbl&gt; 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    &lt;dbl&gt; 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n7.2.5 Group means\n\n# We use as.data.frame() to get more decimals\n\ngss2018 %&gt;% \n  group_by(female) %&gt;%\n  summarise(group_mean = mean(pincome)) %&gt;%\n  as.data.frame()\n\n  female group_mean\n1 female    8.22135\n2   male   10.88184\n\n\n\n\n7.2.6 Regression (as per textbook)\nEven though don’t specify it, the lm() command below use the treatment (or dummy) contrast. That is the default contrast that is used if we don’t indicate the contrast. Better practice would be to explicitly specify the contrast with the command that has been commented out.\n\n# options(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\nlm7_1 &lt;- lm(pincome ~ female,\n            data = gss2018)\n\nUse tidy() to see the weights:\n\ntidy(lm7_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.221350\n0.2479510\n33.157156\n0\n\n\nfemalemale\n2.660494\n0.3697567\n7.195257\n0\n\n\n\n\n\nCompare the weights to values in the graph:\n\n\n\n\n\n\nggplot(data = gss2018,\n       mapping = aes(x = female,\n                     y = pincome)) +\n  stat_summary(fun = \"mean\",\n               geom=\"bar\") +\n  scale_y_continuous(breaks = seq(0, 15)) +\n  theme_classic(12)\n\n\n\n7.2.7 Explanation Part 1: Fewer Cases\nLet’s look at a subset of the data to understand what happened above. We’ll look at the first 20 rows of the data, and do the analysis again.\n\ngss_small &lt;- gss2018 %&gt;%\n  select(pincome, female) %&gt;%\n  slice_head(n = 20)\n\n\nprint(gss_small)\n\n# A tibble: 20 × 2\n   pincome female\n     &lt;dbl&gt; &lt;fct&gt; \n 1      11 male  \n 2       0 female\n 3      22 male  \n 4      23 female\n 5       0 male  \n 6       0 female\n 7      12 female\n 8      17 male  \n 9       2 female\n10      22 male  \n11      23 male  \n12      12 female\n13       0 female\n14      22 male  \n15       0 female\n16       9 female\n17      20 female\n18       0 female\n19      20 female\n20      12 male  \n\n\n\ngss_small &lt;- gss_small %&gt;%\n  mutate(female = as_factor(female)) %&gt;%\n  mutate(female = relevel(female, ref = \"female\"))\n\nLet’s check out the group means for this smaller data set:\n\n# We use as.data.frame() to get more decimals\n\ngss_small %&gt;% \n  group_by(female) %&gt;%\n  summarise(group_mean = mean(pincome)) %&gt;%\n  as.data.frame()\n\n  female group_mean\n1 female   8.166667\n2   male  16.125000\n\n\nNow let’s do the regression:\n\nlm7_small &lt;- lm(pincome ~ female,\n            data = gss_small)\n\n\ntidy(lm7_small)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.166667\n2.506050\n3.258781\n0.0043602\n\n\nfemalemale\n7.958333\n3.962413\n2.008456\n0.0598397\n\n\n\n\n\nCompare the weights to the values in the graph:\n\n\n\n\n\n\nggplot(data = gss_small,\n       mapping = aes(x = female,\n                     y = pincome)) +\n  stat_summary(fun = mean,\n               geom=\"bar\") +\n  scale_y_continuous(breaks = seq(0, 20)) +\n  theme_classic(12)\n\n\n\n7.2.8 Explanation Part 2: Numeric Coding\nWe create a column with 0 for female and 1 for male.\n\ngss_numeric_small &lt;- gss_small\n\ngss_numeric_small$reference_female &lt;-0\n\nid &lt;- gss_numeric_small$female == \"male\"\ngss_numeric_small$reference_female[id] &lt;- 1\n\nprint(gss_numeric_small)\n\n# A tibble: 20 × 3\n   pincome female reference_female\n     &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;\n 1      11 male                  1\n 2       0 female                0\n 3      22 male                  1\n 4      23 female                0\n 5       0 male                  1\n 6       0 female                0\n 7      12 female                0\n 8      17 male                  1\n 9       2 female                0\n10      22 male                  1\n11      23 male                  1\n12      12 female                0\n13       0 female                0\n14      22 male                  1\n15       0 female                0\n16       9 female                0\n17      20 female                0\n18       0 female                0\n19      20 female                0\n20      12 male                  1\n\n\nNow let’s do the regression again!\n\nlm7_numeric_small &lt;- lm(pincome ~ reference_female,\n            data = gss_numeric_small)\n\n\ntidy(lm7_numeric_small)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.166667\n2.506050\n3.258781\n0.0043602\n\n\nreference_female\n7.958333\n3.962413\n2.008456\n0.0598397\n\n\n\n\n\nComapre the weights to the values in the graph:\n\n\n\n\n\n\nggplot(data = gss_numeric_small,\n       mapping = aes(x = reference_female,\n                     y = pincome)) +\n  stat_summary(fun = mean,\n               geom=\"bar\") +\n  scale_y_continuous(breaks = seq(0, 20)) +\n  theme_classic(12)"
  },
  {
    "objectID": "chapter7.html#page-120-single-categorical-t-test",
    "href": "chapter7.html#page-120-single-categorical-t-test",
    "title": "7  Indicator Variables",
    "section": "7.3 Page 120 Single Categorical t-test",
    "text": "7.3 Page 120 Single Categorical t-test\n\nt.test(pincome ~ female, gss2018)\n\n\n    Welch Two Sample t-test\n\ndata:  pincome by female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n# We use as.data.frame() to get more decimals\n\ngss2018 %&gt;% \n  group_by(female) %&gt;%\n  summarise(group_mean = mean(pincome)) %&gt;%\n  as.data.frame()\n\n  female group_mean\n1 female    8.22135\n2   male   10.88184"
  },
  {
    "objectID": "chapter7.html#page-122-multiple-categorical-predictor",
    "href": "chapter7.html#page-122-multiple-categorical-predictor",
    "title": "7  Indicator Variables",
    "section": "7.4 Page 122 Multiple Categorical Predictor",
    "text": "7.4 Page 122 Multiple Categorical Predictor\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(female = as_factor(female)) %&gt;%\n  mutate(female = relevel(female, ref = \"female\"))\n\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(ethnic = as_factor(ethnic)) %&gt;%\n  mutate(ethnic = relevel(ethnic, ref = \"White\"))\n\n\nlm7_2 &lt;- lm(pincome ~ female + ethnic, data = gss2018)\n\n\nlibrary(tidymodels)\ntidy(lm7_2)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.3367516\n0.2861811\n29.1310335\n0.0000000\n\n\nfemalemale\n2.6420425\n0.3700470\n7.1397494\n0.0000000\n\n\nethnicLatinx\n-0.4059728\n0.5294468\n-0.7667867\n0.4432867\n\n\nethnicAfricanAmerican\n-0.6046098\n0.5180019\n-1.1671961\n0.2432516\n\n\nethnicOther\n0.9926937\n0.8669343\n1.1450622\n0.2523020\n\n\n\n\n\nThe intercept corresponds to the cell composed of both reference groups (White/Female). Each weight is the change associated that category membership. Each cell mean (excluding combined reference group: white/female) is obtained by combining multiple regression weights.\n\n\n\n\n\n\n# We use as.data.frame() to get more decimals\n\ncell_means &lt;- gss2018 %&gt;% \n  group_by(female, ethnic) %&gt;%\n  summarise(pincome_mean = mean(pincome)) %&gt;%\n  as.data.frame()\n\n`summarise()` has grouped output by 'female'. You can override using the\n`.groups` argument.\n\nprint(cell_means)\n\n  female          ethnic pincome_mean\n1 female           White     8.345818\n2 female          Latinx     7.373134\n3 female AfricanAmerican     8.447619\n4 female           Other     8.596774\n5   male           White    10.968345\n6   male          Latinx    11.356643\n7   male AfricanAmerican     9.392157\n8   male           Other    12.880000\n\n\n\ncell_means &lt;- cell_means %&gt;%\n  unite(\"cell_label\", female, ethnic)\n\nprint(cell_means)\n\n              cell_label pincome_mean\n1           female_White     8.345818\n2          female_Latinx     7.373134\n3 female_AfricanAmerican     8.447619\n4           female_Other     8.596774\n5             male_White    10.968345\n6            male_Latinx    11.356643\n7   male_AfricanAmerican     9.392157\n8             male_Other    12.880000\n\n\n\nggplot(data = cell_means,\n       mapping = aes(x = cell_label,\n                     y = pincome_mean)) +\n  geom_col() +\n  scale_y_continuous(breaks = seq(0, 16, by = 1)) + \n  coord_cartesian(ylim = c(0, 15)) +\n  theme(axis.text.x = element_text(angle = 60, \n                                   hjust = 1))"
  },
  {
    "objectID": "chapter7.html#page-125-two-categorical-predictors-ancova",
    "href": "chapter7.html#page-125-two-categorical-predictors-ancova",
    "title": "7  Indicator Variables",
    "section": "7.5 Page 125 Two Categorical Predictors (ANCOVA)",
    "text": "7.5 Page 125 Two Categorical Predictors (ANCOVA)\nNotice the fund column in the glimpse output. Notice that is a chr column.\n\ngss2018 %&gt;%\n  select(sort(names(gss2018))) %&gt;%\n  glimpse()\n\nRows: 2,315\nColumns: 28\n$ age        &lt;dbl&gt; 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ attend     &lt;dbl&gt; 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ cappunish  &lt;chr&gt; \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ childs     &lt;dbl&gt; 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ civliberty &lt;dbl&gt; 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ cohort     &lt;dbl&gt; 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ confidence &lt;dbl&gt; 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ educate    &lt;dbl&gt; 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ ethnic     &lt;fct&gt; White, White, Latinx, White, AfricanAmerican, White, Africa…\n$ female     &lt;fct&gt; male, female, male, female, male, female, female, male, fem…\n$ fincome    &lt;dbl&gt; 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ fund       &lt;chr&gt; \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ id         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ latinx     &lt;chr&gt; \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ legalmarij &lt;chr&gt; NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ lifesatis  &lt;dbl&gt; NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ marital    &lt;dbl&gt; 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ occprest   &lt;dbl&gt; 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ owngun     &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ partyaff   &lt;dbl&gt; 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ pincome    &lt;dbl&gt; 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ polviews   &lt;dbl&gt; 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ race       &lt;chr&gt; \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ relig      &lt;dbl&gt; 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ sei        &lt;dbl&gt; 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ spanking   &lt;dbl&gt; 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ volunteer  &lt;dbl&gt; 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ watchtv    &lt;dbl&gt; 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\nWe convert it to factor:\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(fund = as_factor(fund))\n\nThe we check out the levels of that factor:\n\ngss2018 %&gt;%\n  pull(fund) %&gt;%\n  fct_count()\n\n# A tibble: 4 × 2\n  f                  n\n  &lt;fct&gt;          &lt;int&gt;\n1 moderate         869\n2 liberal          787\n3 fundamentalist   539\n4 none             120\n\n\nWe see there are four levels to this factor: moderate, liberal, fundamentalist, none. We also see how many people are at each level.\nBut we want there to be only two levels: fundamentalist, not_fundamentalist. How do we do that? We collapse moderate, liberal, none into non_fundamentalist.\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(fund = fct_collapse(fund,\n                             not_fundamentalist = c(\"moderate\",\n                                                    \"liberal\",\n                                                    \"none\") ) )\n\nNow we check the levels of our factor and we can see it worked. The number of people is the same for fundamentalist, 539 people, and all the other groups have been collapsed into a single category called not_fundamentalist with 1776 people.\n\ngss2018 %&gt;%\n  pull(fund) %&gt;%\n  fct_count()\n\n# A tibble: 2 × 2\n  f                      n\n  &lt;fct&gt;              &lt;int&gt;\n1 not_fundamentalist  1776\n2 fundamentalist       539\n\n\nWe can no use fund as predictor that simply indicates fundamentalist or not. We are going to use a treatment contrast for this analysis. Therefore, we need to see the reference group. We set the reference group (i.e., the group that will have it’s mean represented by the intercept) as the non_fundamentalist group.\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(fund = relevel(fund, ref = \"not_fundamentalist\"))\n\nRecall we set “female” the as the reference group for the female variable previously:\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(female = as_factor(female)) %&gt;%\n  mutate(female = relevel(female, ref = \"female\"))\n\nNow we create out linear model predicting FAMILY income represented by fincome. We have two categorical predictors: fund and female. We also have one numerical predictor, educate, which indicates years of education.\n\nlm7_3 &lt;- lm(fincome ~ fund + female, data = gss2018)\n\nWe can see the regression weights with tidy():\n\nlibrary(tidymodels)\n\ntidy(lm7_3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.9768286\n0.0684558\n160.349032\n0.0000000\n\n\nfundfundamentalist\n-0.2120114\n0.1099137\n-1.928889\n0.0538669\n\n\nfemalemale\n0.2026089\n0.0933808\n2.169705\n0.0301307\n\n\n\n\n\nWe can see the confidence intervals for those weights weights with confint():\n\nconfint(lm7_3)\n\n                         2.5 %       97.5 %\n(Intercept)        10.84258737 11.111069911\nfundfundamentalist -0.42755123  0.003528386\nfemalemale          0.01948994  0.385727833\n\n\nAnd the overall summary info with glance(). The \\(R^2\\) value indicates very little variance in family income is accounted for (&lt; 1%) with this model. So although we have weights - the model doesn’t seem to do much.\n\nglance(lm7_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0038381\n0.0029764\n2.231792\n4.453991\n0.011732\n2\n-5141.835\n10291.67\n10314.66\n11515.83\n2312\n2315"
  },
  {
    "objectID": "chapter7.html#page-126-one-continuous-two-categorical-predictors",
    "href": "chapter7.html#page-126-one-continuous-two-categorical-predictors",
    "title": "7  Indicator Variables",
    "section": "7.6 Page 126 One Continuous Two Categorical Predictors",
    "text": "7.6 Page 126 One Continuous Two Categorical Predictors\nNow we add education (years of education) as a continuous predictor:\n\nlm7_4 &lt;- lm(fincome ~ fund + female + educate, data = gss2018)\n\nWe can see the regression weights with tidy():\n\nlibrary(tidymodels)\n\ntidy(lm7_4)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n7.9564748\n0.2202198\n36.1296903\n0.0000000\n\n\nfundfundamentalist\n-0.0364600\n0.1060401\n-0.3438321\n0.7310038\n\n\nfemalemale\n0.2264217\n0.0895051\n2.5297064\n0.0114818\n\n\neducate\n0.2161229\n0.0150425\n14.3674995\n0.0000000\n\n\n\n\n\nWe can see the confidence intervals for those weights weights with confint():\n\nconfint(lm7_4)\n\n                         2.5 %    97.5 %\n(Intercept)         7.52462567 8.3883240\nfundfundamentalist -0.24440363 0.1714837\nfemalemale          0.05090294 0.4019404\neducate             0.18662471 0.2456211\n\n\nAnd the overall summary info with glance():\n\nglance(lm7_4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0855219\n0.0843348\n2.138796\n72.0415\n0\n3\n-5042.804\n10095.61\n10124.34\n10571.55\n2311\n2315"
  },
  {
    "objectID": "chapter7.html#page-129-predicted-means",
    "href": "chapter7.html#page-129-predicted-means",
    "title": "7  Indicator Variables",
    "section": "7.7 Page 129 Predicted Means",
    "text": "7.7 Page 129 Predicted Means\nThe textbook may have used an approach that leads to incorrect values due to the way factors are handled. Put both levels of the the factors in the same data frame for prediction. Then explicity set the factors as before. Otherwise the reference group may get set differenctly than the orignal analysis and this can cause problems (and incorrect predictions).\n\nlibrary(tidyverse)\n\npredict4values &lt;- data.frame(fund = c(\"not_fundamentalist\",\n                                      \"not_fundamentalist\",\n                                      \"fundamentalist\",\n                                      \"fundamentalist\"),\n                             female =  c(\"male\",\n                                         \"female\",\n                                         \"male\",\n                                         \"female\"),\n                             educate = 12)\n\n\npredict4values &lt;- predict4values %&gt;%\n  mutate(fund = as_factor(fund)) %&gt;%\n  mutate(fund = relevel(fund, ref = \"not_fundamentalist\")) %&gt;%\n  mutate(female = as_factor(female)) %&gt;%\n  mutate(female = relevel(female, ref = \"female\"))\n\nprint(predict4values)\n\n                fund female educate\n1 not_fundamentalist   male      12\n2 not_fundamentalist female      12\n3     fundamentalist   male      12\n4     fundamentalist female      12\n\n\nWe will now create predicted value for the predictor values above:\n\npredicted_fincome &lt;- predict(lm7_4, predict4values)\n\n\npredict4values &lt;- predict4values %&gt;%\n  mutate(predicted_mean_adj_4_edu = predicted_fincome)\n\nprint(predict4values)\n\n                fund female educate predicted_mean_adj_4_edu\n1 not_fundamentalist   male      12                 10.77637\n2 not_fundamentalist female      12                 10.54995\n3     fundamentalist   male      12                 10.73991\n4     fundamentalist female      12                 10.51349\n\n\nCalculating the actual means:\n\nactual_means &lt;- gss2018 %&gt;%\n  group_by(fund, female) %&gt;%\n  summarise(group_mean = mean(fincome))\n\n`summarise()` has grouped output by 'fund'. You can override using the\n`.groups` argument.\n\nprint(actual_means)\n\n# A tibble: 4 × 3\n# Groups:   fund [2]\n  fund               female group_mean\n  &lt;fct&gt;              &lt;fct&gt;       &lt;dbl&gt;\n1 not_fundamentalist female       11.0\n2 not_fundamentalist male         11.2\n3 fundamentalist     female       10.8\n4 fundamentalist     male         10.9\n\n\nNow let’s combine predicted means and actual means into a single table. The full join command will make sure the values line up with the right rows when we combine the tables:\n\nfull_join(predict4values, actual_means)\n\nJoining with `by = join_by(fund, female)`\n\n\n                fund female educate predicted_mean_adj_4_edu group_mean\n1 not_fundamentalist   male      12                 10.77637   11.19515\n2 not_fundamentalist female      12                 10.54995   10.96320\n3     fundamentalist   male      12                 10.73991   10.90741\n4     fundamentalist female      12                 10.51349   10.80495"
  },
  {
    "objectID": "chapter7.html#page-129-martial-status-and-family-income",
    "href": "chapter7.html#page-129-martial-status-and-family-income",
    "title": "7  Indicator Variables",
    "section": "7.8 Page 129 Martial Status and Family Income",
    "text": "7.8 Page 129 Martial Status and Family Income\nWe select the maritial column and then glimpse it.\n\ngss2018 %&gt;%\n  select(marital) %&gt;%\n  glimpse()\n\nRows: 2,315\nColumns: 1\n$ marital &lt;dbl&gt; 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4, 1,…\n\n\nWe see the values 1, 2, 3, 4. Not very helpful. We know that 1 = married, 2 = widowed, 3 = divorced or separated, 4 = never married. These numbers are ordinal - but the factor is really categorical and not ordinal. We fix this by turning the numbers into factors.\n\ngss2018 &lt;- gss2018 %&gt;%\n  mutate(marital = as_factor(marital)) %&gt;%\n  mutate(marital = fct_recode(marital,\n                              married = \"1\",\n                              widowed = \"2\",\n                              div_sep = \"3\",\n                              never_married = \"4\"))\n\nLet’s see if it’s a factor that usings the new labels:\n\ngss2018 %&gt;%\n  select(marital) %&gt;%\n  glimpse()\n\nRows: 2,315\nColumns: 1\n$ marital &lt;fct&gt; never_married, div_sep, married, married, div_sep, widowed, di…\n\n\nNow let’s see the number at each level of the factor:\n\ngss2018 %&gt;%\n  pull(marital) %&gt;%\n  fct_count()\n\n# A tibble: 4 × 2\n  f                 n\n  &lt;fct&gt;         &lt;int&gt;\n1 married         981\n2 widowed         199\n3 div_sep         472\n4 never_married   663\n\n\nNow the regression:\n\nlm7_5 &lt;- lm(fincome ~ marital, data = gss2018)\n\nUse glance() to see the summary. Remember glance() is from the tidymodels package.\n\nglance(lm7_5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0712264\n0.0700207\n2.155449\n59.07578\n0\n3\n-5060.758\n10131.52\n10160.25\n10736.81\n2311\n2315\n\n\n\n\n\nUse tidy() to see the weights:\n\ntidy(lm7_5)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.6748216\n0.0688182\n169.647353\n0\n\n\nmaritalwidowed\n-0.9713040\n0.1675782\n-5.796123\n0\n\n\nmaritaldiv_sep\n-0.8464318\n0.1207438\n-7.010145\n0\n\n\nmaritalnever_married\n-1.3972952\n0.1083671\n-12.894095\n0\n\n\n\n\n\nWe can use confint() to see the CI’s on the weights:\n\nconfint(lm7_5)\n\n                         2.5 %     97.5 %\n(Intercept)          11.539870 11.8097734\nmaritalwidowed       -1.299923 -0.6426846\nmaritaldiv_sep       -1.083209 -0.6096542\nmaritalnever_married -1.609802 -1.1847884"
  },
  {
    "objectID": "chapter7.html#page-131-martial-status-family-income-education-age",
    "href": "chapter7.html#page-131-martial-status-family-income-education-age",
    "title": "7  Indicator Variables",
    "section": "7.9 Page 131 Martial Status, Family Income, Education, Age",
    "text": "7.9 Page 131 Martial Status, Family Income, Education, Age\n\nlm7_6 &lt;- lm(fincome ~ marital + age + educate, data = gss2018)\n\nUse glance() to see the summary. Remember glance() is from the tidymodels package.\n\nglance(lm7_6)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.1441545\n0.1423012\n2.069991\n77.78339\n0\n5\n-4966.104\n9946.207\n9986.437\n9893.746\n2309\n2315\n\n\n\n\n\nUse tidy() to see the weights:\n\ntidy(lm7_6)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.1880161\n0.2635322\n34.864874\n0.0000000\n\n\nmaritalwidowed\n-0.6123468\n0.1747161\n-3.504810\n0.0004656\n\n\nmaritaldiv_sep\n-0.6918057\n0.1172288\n-5.901328\n0.0000000\n\n\nmaritalnever_married\n-1.4055785\n0.1140496\n-12.324279\n0.0000000\n\n\nage\n-0.0068112\n0.0029634\n-2.298465\n0.0216243\n\n\neducate\n0.2009513\n0.0145513\n13.809848\n0.0000000\n\n\n\n\n\nWe can use confint() to see the CI’s on the weights:\n\nconfint(lm7_6)\n\n                           2.5 %       97.5 %\n(Intercept)           8.67123167  9.704800598\nmaritalwidowed       -0.95496372 -0.269729858\nmaritaldiv_sep       -0.92169052 -0.461920943\nmaritalnever_married -1.62922880 -1.181928290\nage                  -0.01262235 -0.001000059\neducate               0.17241630  0.229486281"
  },
  {
    "objectID": "chapter7.html#page-133-chapter-exercises",
    "href": "chapter7.html#page-133-chapter-exercises",
    "title": "7  Indicator Variables",
    "section": "7.10 Page 133 Chapter Exercises",
    "text": "7.10 Page 133 Chapter Exercises\n\n7.10.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n7.10.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Opinion.csv\",\n                save_as = \"opinion.csv\")\n\n\n7.10.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nopinion &lt;- read_csv(\"opinion.csv\") %&gt;% \n  clean_names()\n\n\n\n\n7.10.3 Inspect data\n\nopinion %&gt;% \n  glimpse()  \n\nRows: 1,604\nColumns: 10\n$ id               &lt;dbl&gt; 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 20,…\n$ female           &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,…\n$ educate          &lt;dbl&gt; 18, 18, 16, 18, 18, 20, 20, 19, 15, 15, 15, 18, 16, 1…\n$ african_american &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,…\n$ other_ethnic     &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…\n$ latinx           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,…\n$ income           &lt;dbl&gt; 12, 10, 11, 6, 16, 13, 9, 3, 8, 14, 12, 19, 12, 16, 1…\n$ immigrant        &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ same_opinions    &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,…\n$ candidate_same   &lt;dbl&gt; 18.732189, 18.358946, 21.590763, 19.313181, 7.678196,…"
  },
  {
    "objectID": "chapter8.html#required-packages",
    "href": "chapter8.html#required-packages",
    "title": "8  Independence",
    "section": "8.1 Required packages",
    "text": "8.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\ntidymodels\n\n\nsurvey\n\n\nolsrr\n\n\ncar"
  },
  {
    "objectID": "chapter8.html#page-140-independencedependence-with-means",
    "href": "chapter8.html#page-140-independencedependence-with-means",
    "title": "8  Independence",
    "section": "8.2 Page 140 Independence/Dependence with Means",
    "text": "8.2 Page 140 Independence/Dependence with Means\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv()\nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\nlibrary(tidymodels) #tidy() glance()\nlibrary(survey) \n\n\n8.2.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/MultiLevel.csv\",\n                save_as = \"data_multilevel.csv\")\n\n\n\n8.2.2 Load data from your computer\n\ndata_multilevel &lt;- read_csv(\"data_multilevel.csv\") %&gt;% \n  clean_names()\n\n\n\n8.2.3 Inspect data\n\ndata_multilevel %&gt;% \n  glimpse()  \n\nRows: 9,859\nColumns: 14\n$ idcomm       &lt;dbl&gt; 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 2…\n$ id           &lt;dbl&gt; 10001, 10004, 10005, 10008, 10009, 10012, 10013, 10015, 1…\n$ male         &lt;chr&gt; \"female\", \"female\", \"male\", \"female\", \"male\", \"female\", \"…\n$ white        &lt;chr&gt; \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wh…\n$ married      &lt;chr&gt; \"no\", \"yes\", \"yes\", \"no\", \"yes\", \"no\", \"yes\", \"no\", \"yes\"…\n$ age          &lt;dbl&gt; 42, 68, 58, 37, 64, 69, 33, 74, 41, 47, 41, 56, 33, 71, 5…\n$ educate      &lt;dbl&gt; 5, 3, 3, 5, 4, 3, 3, 6, 6, 3, 3, 4, 6, 3, 4, 3, 4, 7, 3, …\n$ income       &lt;dbl&gt; 2, 4, 3, 2, 8, 3, 6, 3, 7, 2, 4, 4, 6, 7, 6, 8, 4, 4, 4, …\n$ commlength   &lt;dbl&gt; 42, 50, 50, 5, 30, 69, 9, 30, 11, 8, 30, 5, 2, 71, 25, 2,…\n$ commtrust    &lt;dbl&gt; 0.349, -0.434, -1.546, -0.707, -0.264, 0.465, -0.596, 0.5…\n$ trust        &lt;chr&gt; \"not_high\", \"not_high\", \"not_high\", \"not_high\", \"not_high…\n$ pop2020      &lt;dbl&gt; 917, 917, 917, 917, 917, 917, 917, 917, 917, 917, 917, 91…\n$ relighomog   &lt;dbl&gt; 0.277, 0.277, 0.277, 0.277, 0.277, 0.277, 0.277, 0.277, 0…\n$ disadvantage &lt;dbl&gt; -1.219, -1.219, -1.219, -1.219, -1.219, -1.219, -1.219, -…\n\n\n\n\n8.2.4 View data\nRemember you can always use the view() command in R to see the data in a spreadsheet:\n\nview(data_multilevel)\n\n\n\n8.2.5 Specify Design\nWe indicate town is a grouping variable (idcomm which I think stands for id community).\n\nclus_multilevel &lt;- svydesign(ids = ~idcomm,\n                             data = data_multilevel)\n\nWarning in svydesign.default(ids = ~idcomm, data = data_multilevel): No weights\nor probabilities supplied, assuming equal probability\n\nsummary(clus_multilevel)\n\n1 - level Cluster Sampling design (with replacement)\nWith (99) clusters.\nsvydesign(ids = ~idcomm, data = data_multilevel)\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \nData variables:\n [1] \"idcomm\"       \"id\"           \"male\"         \"white\"        \"married\"     \n [6] \"age\"          \"educate\"      \"income\"       \"commlength\"   \"commtrust\"   \n[11] \"trust\"        \"pop2020\"      \"relighomog\"   \"disadvantage\"\n\n\n\n\n8.2.6 Calculate Mean/SE\nIn the calculation we below we us the is.na() command. This provides a vector of TRUE/FALSE non-missing values. TRUE indicates missing. We using an exclamation mark in from to the INVERT the TRUE/FALSE. That means, TRUE indicates NOT missing after we using the exclamation mark. Then we use the sum() command to add up the TRUEs. Each TRUE counts as 1 so this provides us with a count of non-missing values. We could have used the n() command but this doesn’t know if there are missing values and can produce incorrect results.\n\ndata_multilevel %&gt;%\n  summarise(n = sum(!is.na(commlength)),\n            mean = mean(commlength),\n            SD= sd(commlength),\n            SE = SD/sqrt(n)) %&gt;%\n  as.data.frame()\n\n     n     mean       SD        SE\n1 9859 33.25885 22.44965 0.2260962\n\n\nConsider the SE calculation here. It is the usual one.\n\\[\nSE = \\frac{SD}{\\sqrt{n}}= \\frac{22.44965}{\\sqrt{9859}} = 0.2260962\n\\]\n\n\n8.2.7 Calculate Mean/SE Incorporting Cluster Dependence\n\nsvymean(~commlength,\n        design = clus_multilevel)\n\n             mean     SE\ncommlength 33.259 0.5297\n\n\nNotice how the SE is much larger when clustering is taken into account. When the clusters are taken into account it means that we realize that within a clusters each person does not provide information independent of the other people. So from a statistical perspective this makes the effective sample size smaller when calculating SE. In this data set the 9859 people (with dependence due to clusters) are equivalent to 1796.219 independent people - see the calculation below. Note though this is not the actual calculation “under the hood”, it is just a means of illustrating what we are compensating for when we take clusters into account. The lower effective sample size (due to a lack of independence) results in a larger Standard Error (0.5297 vs 0.2261).\n\\[\nSE = \\frac{SD}{\\sqrt{n}}= \\frac{22.44965}{\\sqrt{1796.219}} = 0.5296999\n\\]"
  },
  {
    "objectID": "chapter8.html#page-142-independencedependence-with-regression",
    "href": "chapter8.html#page-142-independencedependence-with-regression",
    "title": "8  Independence",
    "section": "8.3 Page 142 Independence/Dependence with Regression",
    "text": "8.3 Page 142 Independence/Dependence with Regression\n\n8.3.1 Regular Regression No Adjustment\n\nlm8_1 &lt;- lm(income ~ male + married, \n            data = data_multilevel)\n\n\ntidy(lm8_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.9309727\n0.0363492\n80.63368\n0\n\n\nmalemale\n0.5187255\n0.0394160\n13.16028\n0\n\n\nmarriedyes\n1.7475797\n0.0422214\n41.39084\n0\n\n\n\n\n\n\nconfint(lm8_1)\n\n                2.5 %    97.5 %\n(Intercept) 2.8597208 3.0022247\nmalemale    0.4414621 0.5959889\nmarriedyes  1.6648171 1.8303423\n\n\n\nglance(lm8_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.1810082\n0.180842\n1.908896\n1089.154\n0\n2\n-20361.91\n40731.81\n40760.6\n35914.12\n9856\n9859\n\n\n\n\n\n\n\n8.3.2 Dependence Adjusted Regression\n\nclus_multilevel &lt;- svydesign(ids = ~idcomm,\n                             data = data_multilevel)\n\nWarning in svydesign.default(ids = ~idcomm, data = data_multilevel): No weights\nor probabilities supplied, assuming equal probability\n\nsummary(clus_multilevel)\n\n1 - level Cluster Sampling design (with replacement)\nWith (99) clusters.\nsvydesign(ids = ~idcomm, data = data_multilevel)\nProbabilities:\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       1       1       1       1       1 \nData variables:\n [1] \"idcomm\"       \"id\"           \"male\"         \"white\"        \"married\"     \n [6] \"age\"          \"educate\"      \"income\"       \"commlength\"   \"commtrust\"   \n[11] \"trust\"        \"pop2020\"      \"relighomog\"   \"disadvantage\"\n\n\n\nlm8_2 &lt;- svyglm(income ~ male + married, \n                design = clus_multilevel)\n\n\ntidy(lm8_2)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.9309727\n0.0406152\n72.16442\n0\n\n\nmalemale\n0.5187255\n0.0420681\n12.33060\n0\n\n\nmarriedyes\n1.7475797\n0.0441621\n39.57192\n0\n\n\n\n\n\n\nconfint(lm8_2)\n\n                2.5 %    97.5 %\n(Intercept) 2.8503522 3.0115933\nmalemale    0.4352209 0.6022301\nmarriedyes  1.6599186 1.8352408\n\n\n\nglance(lm8_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnull.deviance\ndf.null\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n43851.62\n9858\n11183.24\n35941.71\n35914.12\n96\n9859\n\n\n\n\n\n\n\n8.3.3 Comparing results\nIn the two outputs below notice the standard error is higher in the Dependence Adjusted results. This follows the same principle as when we dealt with means: when dependence is taken into account the standard error is larger. This correspondingly affects p-values - making them larger (further from 0) but more accurate.\n\n8.3.3.1 Regular regression - see SE for weights\n\ntidy(lm8_1)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.9309727\n0.0363492\n80.63368\n0\n\n\nmalemale\n0.5187255\n0.0394160\n13.16028\n0\n\n\nmarriedyes\n1.7475797\n0.0422214\n41.39084\n0\n\n\n\n\n\n\n\n8.3.3.2 Dependence Adjusted - see SE for weights\n\ntidy(lm8_2)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2.9309727\n0.0406152\n72.16442\n0\n\n\nmalemale\n0.5187255\n0.0420681\n12.33060\n0\n\n\nmarriedyes\n1.7475797\n0.0441621\n39.57192\n0"
  },
  {
    "objectID": "chapter8.html#serial-correlation",
    "href": "chapter8.html#serial-correlation",
    "title": "8  Independence",
    "section": "8.4 Serial Correlation",
    "text": "8.4 Serial Correlation\n\n8.4.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Colombia2000_16.csv\",\n                save_as = \"columbia.csv\")\n\n\n\n8.4.2 Load data from your computer\n\ncolumbia &lt;- read_csv(\"columbia.csv\") %&gt;% \n  clean_names()\n\n\n\n8.4.3 Inspect data\n\ncolumbia %&gt;% \n  glimpse()  \n\nRows: 17\nColumns: 9\n$ index         &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17\n$ year          &lt;dbl&gt; 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 20…\n$ homicide_rate &lt;dbl&gt; 66.5, 68.6, 68.9, 53.8, 44.8, 39.6, 36.8, 34.7, 33.0, 33…\n$ homicides     &lt;dbl&gt; 26540, 27840, 28387, 22526, 19036, 17086, 16119, 15423, …\n$ population    &lt;dbl&gt; 39.64, 40.26, 40.88, 41.48, 42.08, 42.65, 43.20, 43.74, …\n$ unemploy      &lt;dbl&gt; 20.52, 15.04, 15.63, 14.19, 13.72, 11.87, 11.51, 11.20, …\n$ poverty       &lt;dbl&gt; 53.7, 60.5, 53.2, 50.9, 50.8, 46.7, 44.1, 43.4, 42.3, 41…\n$ gni           &lt;dbl&gt; 2340, 2330, 2370, 2370, 2620, 2990, 3500, 4120, 4700, 51…\n$ education     &lt;dbl&gt; 6.50, 6.50, 6.60, 6.65, 6.75, 6.00, 6.90, 7.00, 7.10, 7.…\n\n\n\n\n8.4.4 View data\nRemember you can always use the view() command in R to see the data in a spreadsheet:\n\nview(columbia)\n\n\n\n8.4.5 Regular Regression\n\nlm8_3 &lt;- lm(homicide_rate ~ poverty,\n            data=columbia)\n\n\ntidy(lm8_3)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-18.955563\n7.1414543\n-2.654300\n0.0180357\n\n\npoverty\n1.407943\n0.1660531\n8.478867\n0.0000004\n\n\n\n\n\n\nconfint(lm8_3)\n\n                 2.5 %    97.5 %\n(Intercept) -34.177213 -3.733914\npoverty       1.054009  1.761876\n\n\n\nglance(lm8_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.8273703\n0.8158617\n6.479184\n71.89119\n4e-07\n1\n-54.82418\n115.6484\n118.148\n629.6975\n15\n17\n\n\n\n\n\n\n8.4.5.1 Check Autocorrelation of Residuals\n\nlibrary(olsrr)\n\n# Deleted studentized residual vs fitted values plot\nols_plot_resid_stud_fit(lm8_3)\n\n\n\n\nObtain the residuals using the augment() command from the broom package. The residuals are in the .resid column.\n\naugment(lm8_3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhomicide_rate\npoverty\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n66.5\n53.7\n56.65095\n9.849052\n0.1494618\n6.068935\n0.2387046\n1.6482656\n\n\n68.6\n60.5\n66.22496\n2.375043\n0.2847689\n6.664462\n0.0374001\n0.4334390\n\n\n68.9\n53.2\n55.94698\n12.953024\n0.1419102\n5.568849\n0.3851410\n2.1581640\n\n\n53.8\n50.9\n52.70871\n1.091291\n0.1114027\n6.699452\n0.0020012\n0.1786767\n\n\n44.8\n50.8\n52.56791\n-7.767915\n0.1102340\n6.335170\n0.1000696\n-1.2710019\n\n\n39.6\n46.7\n46.79535\n-7.195350\n0.0736249\n6.402063\n0.0529035\n-1.1538203\n\n\n36.8\n44.1\n43.13470\n-6.334700\n0.0618514\n6.474805\n0.0335883\n-1.0094153\n\n\n34.7\n43.4\n42.14914\n-7.449140\n0.0601989\n6.384431\n0.0450462\n-1.1859540\n\n\n33.0\n42.3\n40.60040\n-7.600404\n0.0589026\n6.371341\n0.0457581\n-1.2092025\n\n\n33.7\n41.1\n38.91087\n-5.210873\n0.0593014\n6.551077\n0.0216727\n-0.8292106\n\n\n32.3\n38.3\n34.96863\n-2.668634\n0.0675883\n6.665796\n0.0065942\n-0.4265449\n\n\n33.6\n35.2\n30.60401\n2.995988\n0.0887765\n6.653930\n0.0114303\n0.4844035\n\n\n30.8\n34.0\n28.91448\n1.885519\n0.1003676\n6.685516\n0.0052512\n0.3068160\n\n\n27.9\n32.0\n26.09860\n1.801404\n0.1238899\n6.686840\n0.0062384\n0.2970376\n\n\n26.5\n29.8\n23.00112\n3.498877\n0.1558335\n6.628916\n0.0318854\n0.5877523\n\n\n25.5\n28.7\n21.45239\n4.047614\n0.1741896\n6.600100\n0.0498413\n0.6874460\n\n\n24.9\n28.5\n21.17080\n3.729203\n0.1776978\n6.615918\n0.0435292\n0.6347167\n\n\n\n\n\nWe can calculate the Durbin-Watson test statistic with the formula below. But that notation can be hard to understand.\n\\[\nd = \\frac{\\sum\\limits_{ti=2}^n(\\epsilon_{ti}-\\epsilon_{ti-1})^2}{\\sum\\limits_{ti=1}^n\\epsilon_{ti}^2}\n\\]\nLet’s consider the numerator first. Look the values in the .resid column and see how they are used.\n\\[\n\\begin{aligned}\n\\sum\\limits_{ti=2}^n(\\epsilon_{ti}-\\epsilon_{ti-1})^2 &= (2.375043 - 9.849052)^2 + (12.953023-2.375043)^2 + (1.091291-12.953023)^2 + ... + (3.729203 - 4.047614) ^2\\\\\n&= 438.059\n\\end{aligned}\n\\]\nNow the denominator:\n\\[\n\\begin{aligned}\n\\sum\\limits_{ti=1}^n\\epsilon_{ti}^2 &= 9.849052^2 +  2.375043^2 + 12.953023^2 + ... + 3.729203^2\\\\\n&= 629.6975\n\\end{aligned}\n\\]\nWe combine them to get the final Durbin-Watson statistic:\n\\[\nd = \\frac{\\sum\\limits_{ti=2}^n(\\epsilon_{ti}-\\epsilon_{ti-1})^2}{\\sum\\limits_{ti=1}^n\\epsilon_{ti}^2} = \\frac{438.059}{629.6975} = 0.6956658\n\\]\nCompare this to the output:\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\ndurbinWatsonTest(lm8_3)\n\n lag Autocorrelation D-W Statistic p-value\n   1       0.5641004     0.6956658       0\n Alternative hypothesis: rho != 0\n\n\nThis output also contains an autocorrelation value. It is calculated using the formula below. Not this is the formula used by R. The formula in the text book on p. 149 Footnote 10 is not used by the R durbinWatsonTest() command.\n\\[\nr = \\frac{\\sum\\limits_{ti=2}^n\\epsilon_{ti}\\epsilon_{ti-1}}{\\sum\\limits_{ti=1}^n\\epsilon_{ti}^2} = 0.5641004\n\\]\nAutocorrelation of residuals is a way of determining if the independence of rows assumption for regression is true. The autocorrelation of residuals is a way of examining if the residuals at time 1 are related to residuals at time 2. The the two are unrelated the rows are independent - and the regression assumption is true.\nConsider the table below. The first column on the left is the original residuals. The second column on the right is the residuals shifted by one spot (i.e., a lag of 1 time period). Autocorrelation of residuals (lag 1) checks to see if these two columns are correlated. If the residuals are correlated (i.e., the columns below are correlated) this indicates the errors at one time are related to the errors at another time. That is, the errors on one row are related to the errors on another row. If this is true, the rows are not independent and the regression assumption is violated.\nThe textbook autcorrelation would give you the actual correlation formula between the two columns below which is .62. For technical R actually uses a slightly different calculation formula, above, which is conceptually similar but produces a value of 0.56.\n\n\n\n\n\nresiduals\nshifted_residuals\n\n\n\n\n2.375043\n9.849052\n\n\n12.953024\n2.375043\n\n\n1.091291\n12.953024\n\n\n-7.767915\n1.091291\n\n\n-7.195350\n-7.767915\n\n\n-6.334700\n-7.195350\n\n\n-7.449140\n-6.334700\n\n\n-7.600404\n-7.449140\n\n\n-5.210873\n-7.600404\n\n\n-2.668634\n-5.210873\n\n\n2.995988\n-2.668634\n\n\n1.885519\n2.995988\n\n\n1.801404\n1.885519\n\n\n3.498877\n1.801404\n\n\n4.047614\n3.498877\n\n\n3.729203\n4.047614"
  }
]