[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Textbook Supplement",
    "section": "",
    "text": "Preface\nThis is a Quarto book designed to make it easier use R code with the textbook examples. In some cases I provide extra R code to make life easier for you (e.g., obtaining data from GitHub). In other cases, I use different code from the book to maintain the code-style we use in the course (i.e., a tidyverse approach to R)."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "No R-code in this chapter."
  },
  {
    "objectID": "chapter2.html#required-packages",
    "href": "chapter2.html#required-packages",
    "title": "2  Review",
    "section": "2.1 Required packages",
    "text": "2.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\npsych\n\n\n\nREMINDER: Never use the command library(psych)."
  },
  {
    "objectID": "chapter2.html#page-12-normal-density-plot",
    "href": "chapter2.html#page-12-normal-density-plot",
    "title": "2  Review",
    "section": "2.2 Page 12 Normal Density Plot",
    "text": "2.2 Page 12 Normal Density Plot\n\n2.2.1 Normal Distribution\n\nlibrary(tidyverse)\n\npopulation_data <- data.frame(weights = rnorm(1000000, 80, 10))\n\nweight_graph <- ggplot(data = population_data,\n                   mapping = aes(x = weights)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Weights\") +\n  scale_y_continuous(name = \"Density\") +\n  theme_classic()\n\nprint(weight_graph)\n\n\n\n\n\n\n2.2.2 Skewed Distribution\n\nlibrary(tidyverse)\n\nincome_data <- data.frame(income = rf(1000000, df1 = 5, df2 = 2000))\n\nincome_graph <- ggplot(data = income_data,\n                   mapping = aes(x = income)) +\n  geom_density() +\n  scale_x_continuous(name = \"Distribution of Income\", \n                     breaks = seq(0,  6, by = 2)) +\n  scale_y_continuous(name = \"Density\",\n                     breaks = seq(0, .8, by = .2)) +\n\n  theme_classic()\n\nprint(income_graph)"
  },
  {
    "objectID": "chapter2.html#page-27-standardized-scores",
    "href": "chapter2.html#page-27-standardized-scores",
    "title": "2  Review",
    "section": "2.3 Page 27 Standardized Scores",
    "text": "2.3 Page 27 Standardized Scores\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nz_sample1_oz <- scale(sample1_oz, center = TRUE, scale = TRUE)\n\n\nsample2_grams <- c(1100, 1150, 1200, 1400, 1700, 1725, 1775)\nz_sample2_grams <- scale(sample2_grams, center = TRUE, scale = TRUE)\n\n\n\n\n\n\nsample1_oz\nsample2_grams\nz_sample1_oz\nz_sample2_grams\n\n\n\n\n40\n1100\n-1.3887301\n-1.1405606\n\n\n45\n1150\n-0.9258201\n-0.9706899\n\n\n50\n1200\n-0.4629100\n-0.8008191\n\n\n55\n1400\n0.0000000\n-0.1213362\n\n\n60\n1700\n0.4629100\n0.8978881\n\n\n65\n1725\n0.9258201\n0.9828235\n\n\n70\n1775\n1.3887301\n1.1526942"
  },
  {
    "objectID": "chapter2.html#page-30-correlation-covariance",
    "href": "chapter2.html#page-30-correlation-covariance",
    "title": "2  Review",
    "section": "2.4 Page 30 Correlation /Covariance",
    "text": "2.4 Page 30 Correlation /Covariance\n\nsample1_oz <- c(40, 45, 50, 55, 60, 65, 70)\nsample1_length = c(31, 33, 37, 38, 42, 45, 48)\n\ncov_sample1 <- cov(sample1_oz, sample1_length)\nprint(cov_sample1)\n\n[1] 66.66667\n\ncorrelation_sample1 <- cor(sample1_oz, sample1_length)\nprint(correlation_sample1)\n\n[1] 0.9950372\n\n\nBut also note:\n\n# covariance is like correlation, but with the standard deviations included\n\ncorrelation_sample1 * sd(sample1_oz) * sd(sample1_length)\n\n[1] 66.66667\n\n# You get the same value as the covariance\nprint(cov_sample1)\n\n[1] 66.66667"
  },
  {
    "objectID": "chapter2.html#page-34-nations-data-inspection",
    "href": "chapter2.html#page-34-nations-data-inspection",
    "title": "2  Review",
    "section": "2.5 Page 34 Nations Data Inspection",
    "text": "2.5 Page 34 Nations Data Inspection\n\n2.5.1 Activate packages\n\nlibrary(usethis) # use_github_file() \nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim()\n\n\n\n2.5.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n2.5.2.1 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n\n2.5.3 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\nOr use\n\nnations2018 %>% \n  view()  \n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n\n2.5.4 Descriptive with skim()\n\nlibrary(skimr)\n\nnations2018 %>% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n8\n\n\nNumber of columns\n4\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nnation\n0\n1\n5\n14\n0\n8\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nexpend\n0\n1\n19.79\n2.87\n14.1\n18.88\n19.80\n21.42\n23.4\n▂▁▅▇▅\n\n\neconopen\n0\n1\n59.05\n19.87\n27.1\n52.98\n61.75\n67.42\n87.4\n▅▁▇▂▅\n\n\nperlabor\n0\n1\n24.55\n16.72\n8.8\n14.90\n20.20\n28.02\n60.3\n▇▃▂▁▂\n\n\n\n\n\n\n\n2.5.5 Descriptives with describe()\nAlternatively, you could use the describe() command from the psych package as per the book. But, NEVER use library(psych) it will break the tidyverse. Instead use psyc:: before each psych package command.\n\n# psych package must be installed. But do not use library(psych)\n# Notice how psych creates a mean for the nation column - which makes no sense\nnations2018 %>% \n  psych::describe()  \n\n         vars n  mean    sd median trimmed   mad  min  max range  skew kurtosis\nnation*     1 8  4.50  2.45   4.50    4.50  2.97  1.0  8.0   7.0  0.00    -1.65\nexpend      2 8 19.79  2.87  19.80   19.79  1.85 14.1 23.4   9.3 -0.60    -0.62\neconopen    3 8 59.05 19.87  61.75   59.05 12.75 27.1 87.4  60.3 -0.31    -1.29\nperlabor    4 8 24.55 16.72  20.20   24.55 11.71  8.8 60.3  51.5  1.04    -0.19\n           se\nnation*  0.87\nexpend   1.01\neconopen 7.02\nperlabor 5.91"
  },
  {
    "objectID": "chapter2.html#page-34-gss-t-test",
    "href": "chapter2.html#page-34-gss-t-test",
    "title": "2  Review",
    "section": "2.6 Page 34 GSS t-test",
    "text": "2.6 Page 34 GSS t-test\n\nlibrary(usethis) # use_github_file \nlibrary(tidyverse) # read_csv \nlibrary(janitor) # clean_names() \nlibrary(skimr) # skim\n\n\n2.6.0.1 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/GSS2018.csv\",\n                save_as = \"gss2018.csv\")\n\n\n\n2.6.0.2 Load data from your computer\n\ngss2018 <- read_csv(\"gss2018.csv\") %>% \n  clean_names()\n\n\n\n2.6.0.3 Inspect data\n\ngss2018 %>% \n  glimpse()  \n\nRows: 2,315\nColumns: 28\n$ id         <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ female     <chr> \"male\", \"female\", \"male\", \"female\", \"male\", \"female\", \"fema…\n$ age        <dbl> 43, 74, 42, 63, 71, 67, 59, 43, 62, 55, 59, 34, 61, 44, 41,…\n$ cohort     <dbl> 1975, 1944, 1976, 1955, 1947, 1951, 1959, 1975, 1956, 1963,…\n$ race       <chr> \"White\", \"White\", \"White\", \"White\", \"AfricanAmerican\", \"Whi…\n$ latinx     <chr> \"no\", \"no\", \"yes\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"…\n$ ethnic     <chr> \"White\", \"White\", \"Latinx\", \"White\", \"AfricanAmerican\", \"Wh…\n$ educate    <dbl> 14, 10, 16, 16, 18, 16, 13, 12, 8, 12, 19, 14, 13, 16, 12, …\n$ childs     <dbl> 0, 3, 2, 2, 0, 2, 6, 0, 4, 2, 2, 3, 2, 2, 2, 4, 0, 2, 2, 0,…\n$ marital    <dbl> 4, 3, 1, 1, 3, 2, 3, 4, 2, 1, 3, 3, 3, 1, 4, 2, 1, 1, 3, 4,…\n$ fincome    <dbl> 11, 12, 12, 13, 10, 10, 10, 12, 5, 12, 12, 11, 11, 12, 2, 1…\n$ pincome    <dbl> 11, 0, 22, 23, 0, 0, 12, 17, 2, 22, 23, 12, 0, 22, 0, 9, 20…\n$ sei        <dbl> 65.30, 14.80, 83.40, 69.30, 68.60, 69.30, 24.20, 23.70, 21.…\n$ occprest   <dbl> 47, 22, 61, 59, 53, 53, 48, 35, 35, 39, 72, 35, 45, 72, 28,…\n$ attend     <dbl> 5, 2, 2, 6, 8, 4, 7, 7, 0, 2, 4, 5, 0, 3, 0, 7, 1, 0, 4, 5,…\n$ relig      <dbl> 1, 2, 6, 1, 2, 2, 1, 2, 6, 1, 2, 1, 2, 2, 6, 2, 6, 4, 1, 2,…\n$ fund       <chr> \"moderate\", \"moderate\", \"liberal\", \"liberal\", \"moderate\", \"…\n$ owngun     <chr> \"no\", \"no\", \"no\", \"no\", \"yes\", \"yes\", \"no\", \"no\", \"no\", \"no…\n$ legalmarij <chr> NA, \"no\", \"yes\", \"no\", \"no\", NA, \"yes\", \"yes\", \"yes\", \"yes\"…\n$ cappunish  <chr> \"yes\", \"no\", \"yes\", \"no\", \"no\", \"yes\", \"yes\", \"yes\", \"yes\",…\n$ partyaff   <dbl> 6, 3, 5, 3, 7, 3, 1, 6, 4, 2, 7, 2, 2, 1, 5, 4, 3, 4, 2, 5,…\n$ polviews   <dbl> 6, 4, 5, 4, 7, 3, 4, 5, 4, 4, 6, 4, 4, 3, 2, 5, 2, 6, 2, 4,…\n$ spanking   <dbl> 2, NA, 2, 3, NA, 3, NA, NA, 1, 2, 3, NA, 3, NA, 3, 2, 3, 3,…\n$ lifesatis  <dbl> NA, 87.91, NA, 78.23, 77.39, NA, 72.31, 80.96, NA, 71.21, N…\n$ volunteer  <dbl> 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 1, 5, 1,…\n$ confidence <dbl> 0, 10, 3, 3, 7, 1, 4, 2, 1, 4, 4, 1, 1, 3, 3, 1, 0, 3, 2, 3…\n$ civliberty <dbl> 12, 11, 0, 0, 12, 12, 10, 6, 0, 0, 12, 0, 5, 4, 0, 9, 0, 3,…\n$ watchtv    <dbl> 3, NA, 1, 1, NA, 8, NA, NA, 4, 2, 3, 3, 7, NA, 7, 5, 3, 1, …\n\n\n\n\n2.6.0.4 skimr(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2315\n\n\nNumber of columns\n2\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nfemale\n0\n1\n4\n6\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npincome\n0\n1\n9.42\n8.95\n0\n0\n9\n18\n26\n▇▂▂▅▂\n\n\n\n\n\n\n\n2.6.0.5 psyc::describe(): Describe focal variables\n\nlibrary(skimr)\n\ngss2018 %>% \n  select(pincome, female) %>%\n  psych::describe()  \n\n        vars    n mean   sd median trimmed   mad min max range skew kurtosis\npincome    1 2315 9.42 8.95      9    8.85 13.34   0  26    26 0.21    -1.55\nfemale*    2 2315 1.45 0.50      1    1.44  0.00   1   2     1 0.20    -1.96\n          se\npincome 0.19\nfemale* 0.01\n\n\n\n\n2.6.0.6 Standard t.test\n\nt.test(gss2018$pincome ~ gss2018$female )\n\n\n    Welch Two Sample t-test\n\ndata:  gss2018$pincome by gss2018$female\nt = -7.1248, df = 2123.5, p-value = 1.422e-12\nalternative hypothesis: true difference in means between group female and group male is not equal to 0\n95 percent confidence interval:\n -3.392792 -1.928197\nsample estimates:\nmean in group female   mean in group male \n             8.22135             10.88184 \n\n\n\n\n2.6.0.7 apaText t.test\n\nlibrary(apaText)\n\n# This code provides markdown text for Quarto documents\n\ngss2018 %>%\n  mutate(female = as.factor(female)) %>%\n  apa.ind.t.test(female, pincome, var.equal = FALSE)\n\n[1] \"$\\\\Delta M$ = 2.66, 95% CI[1.93, 3.39], *t*(2123.48) = 7.12, *p* < .001\""
  },
  {
    "objectID": "chapter2.html#page-35-chapter-exercises",
    "href": "chapter2.html#page-35-chapter-exercises",
    "title": "2  Review",
    "section": "2.7 Page 35 Chapter Exercises",
    "text": "2.7 Page 35 Chapter Exercises\n\n2.7.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n2.7.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/American.csv\",\n                save_as = \"americans.csv\")\n\n\n\n2.7.3 Load data from your computer\n\namericans <- read_csv(\"americans.csv\") %>% \n  clean_names()\n\n\n\n2.7.4 Inspect data\n\namericans %>% \n  glimpse()  \n\nRows: 2,797\nColumns: 4\n$ id       <dbl> 1030, 1059, 1076, 1079, 1084, 1092, 1102, 1110, 1115, 1117, 1…\n$ educate  <dbl> 14, 14, 10, 16, 14, 14, 12, 14, 16, 16, 14, 20, 12, 20, 14, 1…\n$ american <dbl> 2.00000, 14.00000, 20.00000, 10.00000, 12.50000, 26.66667, 4.…\n$ group    <chr> \"Not immigrant\", \"Not immigrant\", \"Not immigrant\", \"Not immig…\n\n\n\n\n2.7.5 Hints\n\nlibrary(tidyverse)\n\nglimpse(americans)\n\nRows: 2,797\nColumns: 4\n$ id       <dbl> 1030, 1059, 1076, 1079, 1084, 1092, 1102, 1110, 1115, 1117, 1…\n$ educate  <dbl> 14, 14, 10, 16, 14, 14, 12, 14, 16, 16, 14, 20, 12, 20, 14, 1…\n$ american <dbl> 2.00000, 14.00000, 20.00000, 10.00000, 12.50000, 26.66667, 4.…\n$ group    <chr> \"Not immigrant\", \"Not immigrant\", \"Not immigrant\", \"Not immig…\n\n\nNotice the column, group, is a chr column. We need to make it a factor.\n\namericans <- americans %>%\n  mutate(group = as_factor(group))\n\nNow skim() that column. Recall skim() is from the skimr package.\n\namericans %>%\n  select(group) %>%\n  skim()\n\nCompare the reults of the two commands below. Be sure to read the documentation for geom_jitter. Use ?geom_jitter in the console.\n\nggplot(americans, aes(x=educate, y = american)) + \n  geom_point()\n\n\nggplot(americans, aes(x=educate, y = american)) + \n  geom_jitter()"
  },
  {
    "objectID": "chapter3.html#required-packages",
    "href": "chapter3.html#required-packages",
    "title": "3  Simple Linear Regression Models",
    "section": "3.1 Required packages",
    "text": "3.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\napaTables\n\n\nbroom\n\n\npsych\n\n\n\nREMINDER: Never use the command library(psych)."
  },
  {
    "objectID": "chapter3.html#page-40-graphing-introduction",
    "href": "chapter3.html#page-40-graphing-introduction",
    "title": "3  Simple Linear Regression Models",
    "section": "3.2 Page 40 Graphing Introduction",
    "text": "3.2 Page 40 Graphing Introduction\n\n3.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/Nations2018.csv\",\n                save_as = \"nations2018.csv\")\n\n\n\n3.2.3 Load data from your computer\n\nnations2018 <- read_csv(\"nations2018.csv\") %>% \n  clean_names()\n\n\n\n3.2.4 Inspect data\n\nnations2018 %>% \n  glimpse()  \n\nRows: 8\nColumns: 4\n$ nation   <chr> \"Canada\", \"Finland\", \"France\", \"Germany\", \"Italy\", \"Japan\", \"…\n$ expend   <dbl> 21.0, 22.7, 23.4, 19.9, 19.0, 19.7, 18.5, 14.1\n$ econopen <dbl> 64.5, 76.2, 62.7, 87.4, 59.1, 34.6, 60.8, 27.1\n$ perlabor <dbl> 25.9, 60.3, 8.8, 16.5, 34.4, 17.0, 23.4, 10.1\n\n\n\n\n\n\n\nnation\nexpend\neconopen\nperlabor\n\n\n\n\nCanada\n21.0\n64.5\n25.9\n\n\nFinland\n22.7\n76.2\n60.3\n\n\nFrance\n23.4\n62.7\n8.8\n\n\nGermany\n19.9\n87.4\n16.5\n\n\nItaly\n19.0\n59.1\n34.4\n\n\nJapan\n19.7\n34.6\n17.0\n\n\nUnited Kingdom\n18.5\n60.8\n23.4\n\n\nUnited States\n14.1\n27.1\n10.1\n\n\n\n\n\n\n3.2.4.1 Graph\n\nnations_plot <- ggplot(data = nations2018,\n                        mapping = aes(x = perlabor,\n                                      y = expend)) +\n  geom_point(shape = 18) +\n  geom_text(mapping = aes(label = nation),\n            nudge_y = .4) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(5, 65),\n                  ylim = c(14, 24)) +\n  scale_x_continuous(breaks = seq(5, 65, by = 10)) +\n  scale_y_continuous(breaks = seq(14, 24, by = 2)) +\n  labs(x = \"Percent labor union\",\n       y = \"Public expenditures\",\n       title = \"Public Expenditures vs Percent Labor Union\")\n\n  \nprint(nations_plot)\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "chapter3.html#pages-44-to-46-analysis-and-graphing",
    "href": "chapter3.html#pages-44-to-46-analysis-and-graphing",
    "title": "3  Simple Linear Regression Models",
    "section": "3.3 Pages 44 to 46 Analysis and Graphing",
    "text": "3.3 Pages 44 to 46 Analysis and Graphing\n\n3.3.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.3.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n\n3.3.3 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 <- read_csv(\"statedata2018.csv\") %>% \n  clean_names()\n\n\n3.3.3.1 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %>% \n  select(sort(names(statedata2018))) %>%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           <dbl> 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     <dbl> 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               <dbl> 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               <dbl> 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                <dbl> 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       <dbl> 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              <dbl> 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            <chr> \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              <chr> \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      <dbl> 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      <dbl> 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               <dbl> 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                <dbl> 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  <dbl> 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             <dbl> 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            <dbl> 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               <dbl> 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              <dbl> 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  <dbl> 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        <dbl> 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       <dbl> 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            <dbl> 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      <dbl> 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year <dbl> 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   <dbl> 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      <dbl> 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            <dbl> 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           <dbl> 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               <dbl> 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       <dbl> 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            <dbl> 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 <dbl> 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             <dbl> 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              <dbl> 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 <dbl> 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   <dbl> 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   <dbl> 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             <dbl> 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       <dbl> 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        <dbl> 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                <dbl> 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               <dbl> 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               <dbl> 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               <dbl> 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               <dbl> 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              <dbl> 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         <dbl> 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  <dbl> 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  <dbl> 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             <dbl> 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          <dbl> 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 <dbl> 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      <dbl> 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                <dbl> 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  <dbl> 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          <dbl> 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          <dbl> 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                <dbl> 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             <dbl> 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 <dbl> 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                <dbl> 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  <dbl> 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        <dbl> 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            <dbl> 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  <dbl> 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  <dbl> 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               <dbl> 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     <dbl> 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          <dbl> 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     <dbl> 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               <dbl> 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         <dbl> 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        <dbl> 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              <dbl> 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         <dbl> 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        <dbl> 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n\n3.3.4 Graph\n\nstate_plot <- ggplot(data = statedata2018,\n                     mapping = aes(x = life_satis,\n                                   y = opioid_od_death_rate)) +\n  geom_point(shape = 1) +\n  geom_text(mapping = aes(label = state),\n            nudge_y = 1,\n            size = 2) +\n  geom_smooth(method = \"lm\",\n              se = FALSE,\n              color = \"red\") +\n  coord_cartesian(xlim = c(49, 55),\n                  ylim = c(0, 50)) +\n  scale_x_continuous(breaks = seq(49, 55, by = 1)) +\n  scale_y_continuous(breaks = seq(10, 50, by = 10)) +\n  labs(x = \"Average Life Satisfaction\",\n       y = \"Opioid overdose deaths per 100,000\",\n       title = \"Opioid Deaths vs Life Satisfaction\") +\n  theme_light()\n\nprint(state_plot)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n3.3.5 Analysis using summary()\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nsummary(lrm3_1)\n\n\nCall:\nlm(formula = opioid_od_death_rate ~ life_satis, data = statedata2018)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.688  -6.952  -1.511   3.408  28.118 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  212.056     56.576   3.748 0.000479 ***\nlife_satis    -3.792      1.093  -3.468 0.001116 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.292 on 48 degrees of freedom\nMultiple R-squared:  0.2004,    Adjusted R-squared:  0.1837 \nF-statistic: 12.03 on 1 and 48 DF,  p-value: 0.001116\n\nconfint(lrm3_1)\n\n                2.5 %     97.5 %\n(Intercept) 98.302367 325.809877\nlife_satis  -5.989863  -1.593489\n\n\n\n\n3.3.6 Analysis apa.reg.table()\nUsing apaTables to display the regression results is probably a better approach. It’s one step and combines everything into one table. You can get the extra fit information using summary() or the glance() command from the broom package.\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(apaTables)\napa.reg.table(lrm3_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can get extra information (e.g., exact \\(p\\)-values) about specific predictors using:\n\nlibrary(broom)\ntidy(lrm3_1)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   212.       56.6       3.75 0.000479\n2 life_satis     -3.79      1.09     -3.47 0.00112 \n\n\nYou can get extra overall fit information using:\n\nglance(lrm3_1)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.200         0.184  9.29      12.0 0.00112     1  -181.  369.  375.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>"
  },
  {
    "objectID": "chapter3.html#pages-49-to-50-predictedfitted-values",
    "href": "chapter3.html#pages-49-to-50-predictedfitted-values",
    "title": "3  Simple Linear Regression Models",
    "section": "3.4 Pages 49 to 50 Predicted/Fitted Values",
    "text": "3.4 Pages 49 to 50 Predicted/Fitted Values\nWhen you look at the graph with life_satis on the x-axis and opioid_od_death_rate on the y-axis you see a regression line. The points that fall on this line are predicted scores for opioid_od_death_rate based on life_satis. Alternatively, we might call these fitted values for opioid_od_death_rate based on life_satis. We can obtained a predicted score (i.e., fitted score) for each x-axis value using augment() command fromm the broom package. When you inspect the output below only pay attention to the opioid_od_death_rate, life_satis, and .fitted columns. The column pioid_od_death_rate is the measured opiod overdose death rate, the column life_satis is the measured life satisfaction, the column .fitted is the predicted opiod overdose death rate for a given life_satis value.\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\nlibrary(broom)\nlrm3_1 %>%\n  augment()\n\n# A tibble: 50 × 8\n   opioid_od_death_rate life_satis .fitted  .resid   .hat .sigma   .cooksd\n                  <dbl>      <dbl>   <dbl>   <dbl>  <dbl>  <dbl>     <dbl>\n 1                  9         51.4    17.1  -8.08  0.0214   9.31 0.00844  \n 2                 13.9       52.1    14.6  -0.676 0.0217   9.39 0.0000598\n 3                 13.5       51.4    17.2  -3.72  0.0217   9.37 0.00182  \n 4                  6.5       52.3    13.6  -7.07  0.0252   9.33 0.00768  \n 5                  5.3       51.5    16.8 -11.5   0.0208   9.24 0.0167   \n 6                 10         52.9    11.6  -1.56  0.0381   9.39 0.000579 \n 7                 27.7       50.5    20.6   7.08  0.0415   9.33 0.0131   \n 8                 27.8       52.7    12.1  15.7   0.0340   9.10 0.0521   \n 9                 16.3       51.1    18.4  -2.13  0.0262   9.38 0.000726 \n10                  9.7       51.2    17.9  -8.23  0.0240   9.31 0.00988  \n# ℹ 40 more rows\n# ℹ 1 more variable: .std.resid <dbl>\n\n\n\n3.4.1 Fitted values and percentiles\nSee previous section for loading the data.\nWe want to predict opioid overdose at the 25th, 50th, and 75th percentiles for life satisfaction. So we obtain the life satisfaction values corresponding to these percentiles below.\n\nstatedata2018 %>%\n  select(life_satis) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nlife_satis\n0\n1\n51.73\n1.21\n49.07\n51.08\n51.63\n52.33\n55.63\n▃▇▇▂▁\n\n\n\n\n\nFrom the above skim() output we extract the percentile information and put it in a table that’s easy to follow below. We can see the life_satis value for each percentile in this table.\n\n\n\nPercentile\nlife_satis value\n\n\n\n\n25th\n51.1\n\n\n50th\n51.6\n\n\n75th\n52.3\n\n\n\n\n\n3.4.2 Calculate fitted values\n\nlrm3_1 <- lm(opioid_od_death_rate ~ life_satis,\n             data = statedata2018)\n\n\n# we need to use the EXACT name from the original data set\nlife_satis <- c(51.1, 51.6, 52.3)\n\nfit_for_values = data.frame(life_satis)\n\npredict(lrm3_1, fit_for_values)\n\n       1        2        3 \n18.30148 16.40564 13.75146 \n\n\nThe values above are the predicted values for opioid_od_death_rate. We put everything in the table below for clarity. The predicted opioid_od_death_rate value provides a corresponding point on the regression line. That is, all (life_satis value, predicted opioid_od_death_rate value) points fall on the regression line.\n\n\n\n\n\n\n\n\nPercentile life_satis\nlife_satis value\npredicted opioid_od_death_rate value\n\n\n\n\n25th\n51.1\n18.30148\n\n\n50th\n51.6\n16.40564\n\n\n75th\n52.3\n13.75146\n\n\n\nRecall the regression formulas:\n\\[\n\\hat{y} = b_0 + b_1X\n\\]\nIn the context of our variables: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nRecall the regression output:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n212.06**\n[98.30, 325.81]\n\n\n\n\n\n\n\n\nlife_satis\n-3.79**\n[-5.99, -1.59]\n-0.45\n[-0.71, -0.19]\n.20**\n[.04, .38]\n-.45**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .200**\n\n\n\n\n\n\n\n\n\n\n95% CI[.04,.38]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom this output we get the regression formula:\nThis formula \\[\n\\widehat{opioid\\_od\\_death\\_rate} = b_0 + b_1(life\\_satis)\n\\]\nBecomes: \\[\n\\widehat{opioid\\_od\\_death\\_rate} = 212.06 + -3.79(life\\_satis)\n\\]\nTherfore for our three points:\n\\[\n\\begin{aligned}\n18.30148 &= 212.06 + -3.79(51.1)\\\\\n16.40564 &= 212.06 + -3.79(51.6)\\\\\n13.75146 &= 212.06 + -3.79(52.3)\\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html#page-62-chapter-exercises",
    "href": "chapter3.html#page-62-chapter-exercises",
    "title": "3  Simple Linear Regression Models",
    "section": "3.5 Page 62 Chapter Exercises",
    "text": "3.5 Page 62 Chapter Exercises\n\n3.5.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n3.5.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/HighSchool.csv\",\n                save_as = \"highschool.csv\")\n\n\n\n3.5.3 Load data from your computer\n\nhighschool <- read_csv(\"highschool.csv\") %>% \n  clean_names()\n\n\n\n3.5.4 Inspect data\n\nhighschool %>% \n  glimpse()  \n\nRows: 178\nColumns: 6\n$ row                  <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ id_number            <dbl> 2583454, 758606, 6918338, 757890, 4584594, 858964…\n$ sports_participation <dbl> 0.00, 1.10, 0.00, 0.00, 1.10, 1.10, 0.69, 0.00, 1…\n$ academic_clubs       <dbl> 1.61, 0.69, 1.10, 0.00, 0.69, 0.00, 0.69, 1.61, 1…\n$ alcohol_use          <dbl> 1.39, 0.69, 0.69, 0.69, 0.00, 1.10, 1.95, 0.69, 1…\n$ gpa                  <dbl> 3.25, 4.00, 4.00, 2.88, 4.00, 2.25, 2.12, 3.62, 3…\n\n\n\n\n3.5.5 Helpful Commands for Exercise\n\n# horizontal line\ngeom_hline(yintercept = 22)\n\n\n# Challenge help. Use the .resid column in reg_diagnostics\nlibrary(broom)\nreg_diagnostics <- lrm_for_your_data %>% augment()"
  },
  {
    "objectID": "chapter4.html#required-packages",
    "href": "chapter4.html#required-packages",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.1 Required packages",
    "text": "4.1 Required packages\nThe following CRAN packages must be installed:\n\n\n\nRequired CRAN Packages\n\n\n\n\ntidyverse\n\n\nusethis\n\n\njanitor\n\n\nskimr\n\n\napaTables\n\n\nbroom\n\n\ncorrr\n\n\npsych\n\n\n\nREMINDER: Never use the command library(psych)."
  },
  {
    "objectID": "chapter4.html#page-66-correlations",
    "href": "chapter4.html#page-66-correlations",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.2 Page 66 Correlations",
    "text": "4.2 Page 66 Correlations\n\n4.2.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n4.2.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/StateData2018.csv\",\n                save_as = \"statedata2018.csv\")\n\n\n4.2.2.1 Load data from your computer\nClean names is essential here. It makes sure all column names are lower case. They are not all lower case in the original data file.\n\nstatedata2018 <- read_csv(\"statedata2018.csv\") %>% \n  clean_names()\n\n\n\n\n4.2.3 Inspect data\nThere are so many column names in this data set that we do the glimpse a bit differently. That is, we sort the order of the columns alphabetically prior to doing the glimpse(). It affects only the display of the column names - not the structure of the data.\n\nstatedata2018 %>% \n  select(sort(names(statedata2018))) %>%\n  glimpse()  \n\nRows: 50\nColumns: 77\n$ aa_voted_percent           <dbl> 49.6, NA, 50.9, 41.6, 49.9, 31.0, 48.3, 55.…\n$ alc_disorder_past_year     <dbl> 4.51, 6.57, 5.49, 5.38, 5.51, 6.30, 6.10, 6…\n$ assault_rate               <dbl> 283.4, 440.2, 252.1, 346.0, 236.6, 192.8, 1…\n$ assoc_degree               <dbl> 8.2, 8.5, 8.4, 6.7, 7.7, 8.6, 7.5, 7.9, 9.7…\n$ bach_degree                <dbl> 15.4, 19.0, 18.1, 14.2, 20.6, 24.9, 21.9, 1…\n$ binge_alc_past_month       <dbl> 21.40, 24.20, 23.10, 19.49, 23.52, 26.97, 2…\n$ burglary_rate              <dbl> 819.0, 427.6, 647.1, 835.7, 522.3, 438.2, 3…\n$ census_division            <chr> \"EastSouthCentral\", \"Pacific\", \"Mountain\", …\n$ census_region              <chr> \"South\", \"West\", \"West\", \"South\", \"West\", \"…\n$ comm_supervision_rate      <dbl> 1591.14, 1522.95, 1503.95, 2222.08, 1088.59…\n$ community_supervision      <dbl> 60700, 8400, 84800, 51500, 333300, 90900, 4…\n$ conservative               <dbl> 26.00835, 19.27924, 20.40239, 25.04501, 17.…\n$ cost_living                <dbl> 89.3, 129.9, 97.0, 86.9, 151.7, 105.6, 127.…\n$ death_row                  <dbl> 177, 0, 120, 32, 727, 3, 0, 0, 348, 49, 0, …\n$ death_row_rate             <dbl> 4.33, 0.00, 2.18, 1.33, 3.59, 0.09, 0.00, 0…\n$ doctoral_degree            <dbl> 1.0, 1.2, 1.2, 0.9, 1.6, 1.6, 1.6, 1.8, 1.1…\n$ dom_mig_rate               <dbl> 1.17, -14.58, 11.61, 0.82, -3.95, 7.60, -6.…\n$ dom_migration              <dbl> 5718, -10752, 83240, 2475, -156068, 43293, …\n$ fips_code                  <dbl> 1, 2, 4, 5, 6, 8, 9, 10, 12, 13, 15, 16, 17…\n$ gross_state_product        <dbl> 211197, 51479, 326446, 122704, 2797601, 345…\n$ guns                       <dbl> 161641, 15824, 179738, 79841, 344622, 92435…\n$ guns_per_capita            <dbl> 33.15, 21.38, 25.61, 26.57, 8.71, 16.48, 22…\n$ health_exp_per_capita      <dbl> 7281, 11064, 6452, 7408, 7549, 6804, 9859, …\n$ illicit_disorder_past_year <dbl> 2.86, 3.69, 2.79, 2.80, 2.97, 3.43, 3.47, 3…\n$ illicit_drugs_past_month   <dbl> 8.67, 16.81, 10.77, 10.51, 13.11, 17.83, 12…\n$ infant_mortality_rate      <dbl> 9.03, 5.17, 5.32, 8.20, 4.22, 4.76, 4.78, 7…\n$ intern_mig_rate            <dbl> 0.68, 3.26, 2.00, 0.75, 2.98, 1.44, 4.62, 1…\n$ intern_migration           <dbl> 3344, 2401, 14335, 2260, 117797, 8207, 1649…\n$ larceny_rate               <dbl> 2149.5, 2096.4, 2289.1, 2313.5, 1527.4, 185…\n$ latinx_voted_percent       <dbl> 29.1, NA, 48.8, 20.8, 43.3, 43.9, 41.1, NA,…\n$ life_expectancy            <dbl> 75.0, 77.8, 79.2, 75.5, 81.0, 80.0, 80.7, 7…\n$ life_satis                 <dbl> 51.42162, 52.08266, 51.38469, 52.34836, 51.…\n$ masters_degree             <dbl> 6.9, 7.6, 7.8, 5.9, 8.4, 10.9, 12.1, 8.9, 7…\n$ med_hh_income              <dbl> 48486, 76715, 56213, 45726, 71228, 68811, 7…\n$ median_age                 <dbl> 39.0, 33.5, 37.5, 38.0, 36.4, 36.7, 40.9, 4…\n$ mental_illness_past_year   <dbl> 19.95, 20.32, 18.02, 20.73, 18.18, 19.86, 1…\n$ motor_vehicle_theft_rate   <dbl> 209.1, 236.0, 261.3, 188.8, 391.3, 234.8, 1…\n$ murder_ms_rate             <dbl> 5.7, 5.6, 4.7, 5.6, 4.4, 2.8, 2.4, 5.8, 5.8…\n$ opioid_od_death_rate       <dbl> 9.0, 13.9, 13.5, 6.5, 5.3, 10.0, 27.7, 27.8…\n$ pain_pill_past_year        <dbl> 4.53, 4.81, 4.27, 5.03, 4.30, 4.87, 4.21, 4…\n$ per_age0_18                <dbl> 24, 27, 24, 25, 24, 24, 22, 22, 21, 26, 23,…\n$ per_age19_25               <dbl> 9, 9, 9, 9, 9, 9, 8, 8, 8, 9, 7, 9, 9, 9, 9…\n$ per_age26_34               <dbl> 12, 13, 12, 12, 14, 14, 11, 12, 12, 12, 12,…\n$ per_age35_54               <dbl> 25, 26, 24, 25, 26, 26, 27, 25, 25, 27, 25,…\n$ per_age55_64               <dbl> 14, 13, 12, 13, 12, 13, 15, 14, 14, 12, 13,…\n$ per_age65plus              <dbl> 17, 12, 18, 17, 14, 14, 17, 19, 21, 14, 19,…\n$ per_am_ind_alaskan         <dbl> 1.0, 16.0, 4.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.…\n$ per_asian                  <dbl> 1, 6, 3, 2, 15, 3, 5, 4, 3, 4, 38, 2, 6, 2,…\n$ per_black                  <dbl> 26.0, 3.0, 4.0, 15.0, 5.0, 4.0, 10.0, 21.0,…\n$ per_cap_income             <dbl> 26846, 35874, 29265, 25635, 35021, 36415, 4…\n$ per_child_poverty          <dbl> 24, 13, 20, 24, 17, 12, 14, 17, 19, 20, 12,…\n$ per_latinx                 <dbl> 4, 7, 32, 8, 39, 22, 17, 9, 26, 10, 10, 13,…\n$ per_pop_change2010_18      <dbl> 2.254, 3.828, 12.192, 3.354, 6.181, 13.247,…\n$ per_poverty                <dbl> 16.89, 11.11, 14.86, 16.40, 13.30, 10.29, 9…\n$ per_white                  <dbl> 66, 60, 54, 72, 37, 68, 66, 62, 53, 52, 21,…\n$ percent_uninsured          <dbl> 12.0, 14.3, 12.7, 9.8, 8.3, 8.6, 6.2, 6.8, …\n$ pop_change2010_18          <dbl> 107733, 27189, 779358, 97797, 2302522, 6662…\n$ pop_density                <dbl> 95.4, 1.3, 58.3, 56.9, 246.1, 50.8, 742.6, …\n$ pop18and_older             <dbl> 3814879, 551562, 5638481, 2317649, 30617582…\n$ population                 <dbl> 4887871, 737438, 7171646, 3013825, 39557045…\n$ prison_rate                <dbl> 1072.12, 797.73, 975.44, 1035.53, 662.04, 7…\n$ prisoners                  <dbl> 40900, 4400, 55000, 24000, 202700, 32100, 1…\n$ professional_degree        <dbl> 1.5, 1.8, 1.8, 1.3, 2.4, 2.3, 3.0, 1.9, 2.1…\n$ prop_crime_rate            <dbl> 3177.6, 2760.0, 3197.5, 3338.0, 2441.1, 253…\n$ rape_rate                  <dbl> 41.3, 104.7, 50.2, 59.4, 29.7, 56.7, 21.7, …\n$ religious                  <dbl> 32.03850, 24.12306, 26.40519, 30.05092, 24.…\n$ robbery_rate               <dbl> 96.9, 85.4, 92.8, 69.1, 125.5, 56.7, 87.8, …\n$ state                      <chr> \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\",…\n$ state_taxes_per_capita     <dbl> 2262, 2226, 2272, 3266, 4424, 2599, 5341, 4…\n$ suic_rate_age_adj          <dbl> 16.6, 27.0, 18.2, 20.8, 10.5, 20.3, 10.5, 1…\n$ suicide_idea_past_year     <dbl> 3.89, 5.34, 4.02, 4.59, 4.04, 5.41, 4.05, 4…\n$ suicide_rate               <dbl> 15.90, 28.07, 19.37, 21.72, 11.48, 21.59, 1…\n$ tobacco_past_month         <dbl> 30.12, 26.68, 21.19, 30.69, 16.23, 20.15, 2…\n$ total_voted_percent        <dbl> 69.0, 67.7, 68.6, 58.5, 61.5, 65.6, 68.0, 6…\n$ unemploy_rate              <dbl> 4.1, 6.5, 4.7, 3.5, 4.1, 3.1, 4.2, 4.0, 3.5…\n$ violent_crime_rate         <dbl> 427.4, 635.8, 399.9, 480.1, 396.1, 309.1, 2…\n$ white_voted_percent        <dbl> 52.6, 60.3, 65.3, 44.0, 61.4, 62.6, 57.7, 5…\n\n\n\n\n4.2.4 Select focal variables\n\nfocal_data <- statedata2018 %>%\n  select(violent_crime_rate, per_child_poverty, med_hh_income)\n\n\n\n4.2.5 Select focal variables\n\nlibrary(corrr)\n\nfocal_data %>% \n  correlate()\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 4\n  term               violent_crime_rate per_child_poverty med_hh_income\n  <chr>                           <dbl>             <dbl>         <dbl>\n1 violent_crime_rate             NA                 0.492        -0.208\n2 per_child_poverty               0.492            NA            -0.756\n3 med_hh_income                  -0.208            -0.756        NA    \n\n\n\n\n4.2.6 Correlation options\n\n4.2.6.1 psych package\n\nfocal_data %>% \n  psych::corr.test()\n\nCall:psych::corr.test(x = .)\nCorrelation matrix \n                   violent_crime_rate per_child_poverty med_hh_income\nviolent_crime_rate               1.00              0.49         -0.21\nper_child_poverty                0.49              1.00         -0.76\nmed_hh_income                   -0.21             -0.76          1.00\nSample Size \n[1] 50\nProbability values (Entries above the diagonal are adjusted for multiple tests.) \n                   violent_crime_rate per_child_poverty med_hh_income\nviolent_crime_rate               0.00                 0          0.15\nper_child_poverty                0.00                 0          0.00\nmed_hh_income                    0.15                 0          0.00\n\n To see confidence intervals of the correlations, print with the short=FALSE option\n\n\n\n\n4.2.6.2 apaTables package\n\nlibrary(apaTables)\n\nfocal_data %>% \n  apa.cor.table()\n\n\n\nDescriptive Statistics and Correlations\n \n\n  Variable              M        SD      1           2           \n  1. violent_crime_rate 346.81   128.82                          \n                                                                 \n                                                                 \n                                                                 \n  2. per_child_poverty  16.84    4.75    .49**                   \n                                         [.25, .68]              \n                                         p < .001                \n                                                                 \n  3. med_hh_income      60252.12 9879.50 -.21        -.76**      \n                                         [-.46, .07] [-.85, -.61]\n                                         p = .147    p < .001    \n                                                                 \n\nNote. M and SD are used to represent mean and standard deviation, respectively.\nValues in square brackets indicate the 95% confidence interval.\n * indicates p < .05. ** indicates p < .01.\n \n\n\n\n\n4.2.6.3 corrr package\nWe use correlate() to get the correlations, shave() to remove upper diagonal, and fashion() to make it nice:\n\nlibrary(corrr)\n\nfocal_data %>% \n  correlate() %>%\n  shave() %>%\n  fashion()\n\n                term violent_crime_rate per_child_poverty med_hh_income\n1 violent_crime_rate                                                   \n2  per_child_poverty                .49                                \n3      med_hh_income               -.21              -.76              \n\n\nBut more importantly the corr package has network_plot() to visual relations among variables. Here we only plot relations where the magnitude of the correlation is greater than .20:\n\nfocal_data %>% \n  correlate() %>%\n  network_plot(min_cor = .2,\n               colors = c(\"red\", \"green\"), \n               legend = \"full\")"
  },
  {
    "objectID": "chapter4.html#page-67-one-predictor",
    "href": "chapter4.html#page-67-one-predictor",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.3 Page 67 One Predictor",
    "text": "4.3 Page 67 One Predictor\n\nlm4_1 <- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\napa.reg.table(lm4_1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n122.24*\n[2.50, 241.99]\n\n\n\n\n\n\n\n\nper_child_poverty\n13.34**\n[6.49, 20.18]\n0.49\n[0.24, 0.74]\n.24**\n[.06, .42]\n.49**\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .242**\n\n\n\n\n\n\n\n\n\n\n95% CI[.06,.42]"
  },
  {
    "objectID": "chapter4.html#page-68-two-predictors",
    "href": "chapter4.html#page-68-two-predictors",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.4 Page 68 Two Predictors",
    "text": "4.4 Page 68 Two Predictors\n\nlm4_2 <- lm(violent_crime_rate ~ per_child_poverty + med_hh_income,\n            data = focal_data)\n\napa.reg.table(lm4_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npredictor\nb\nb_CI\nbeta\nbeta_CI\nsr2\nsr2_CI\nr\nsummary\n\n\n\n\n(Intercept)\n-310.57\n[-748.95, 127.82]\n\n\n\n\n\n\n\n\nper_child_poverty\n21.18**\n[11.05, 31.31]\n0.78\n[0.41, 1.16]\n.26**\n[.06, .47]\n.49**\n\n\n\nmed_hh_income\n0.00*\n[0.00, 0.01]\n0.38\n[0.01, 0.76]\n.06*\n[-.05, .18]\n-.21\n\n\n\n\n\n\n\n\n\n\n\n\\(R^2\\) = .305**\n\n\n\n\n\n\n\n\n\n\n95% CI[.09,.47]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.1 Two Predictors: Predicted Scores\nTwo help us interpret the data we will make a graph. But to do so we need to know the range for per_child_poverty. We find it ranges from 9 percent to 28 percent from the skim() output below.\n\nfocal_data %>%\n  select(per_child_poverty) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nper_child_poverty\n0\n1\n16.84\n4.75\n9\n13\n16.5\n20\n28\n▅▇▇▅▂\n\n\n\n\n\nNow we want to know the mean of med_hh_income. We find it is 60252.\n\nfocal_data %>%\n  select(per_child_poverty) %>%\n  skim()\n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n50\n\n\nNumber of columns\n1\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nper_child_poverty\n0\n1\n16.84\n4.75\n9\n13\n16.5\n20\n28\n▅▇▇▅▂\n\n\n\n\n\nNow we want to HOLD med_hh_income constant at 60252 see how violent_crime_rate changes with per_child_poverty. We create a dataset where this is the case:\n\npredict4values <- data.frame(per_child_poverty = seq(9, 28), \n                             med_hh_income = 60252)\n\nprint(predict4values)\n\n   per_child_poverty med_hh_income\n1                  9         60252\n2                 10         60252\n3                 11         60252\n4                 12         60252\n5                 13         60252\n6                 14         60252\n7                 15         60252\n8                 16         60252\n9                 17         60252\n10                18         60252\n11                19         60252\n12                20         60252\n13                21         60252\n14                22         60252\n15                23         60252\n16                24         60252\n17                25         60252\n18                26         60252\n19                27         60252\n20                28         60252\n\n\nWe use our regression model to generate predicted scores:\n\n# Create predicted scores use the regression weights created in lm4_2\npredicted_violent_crime_rate <- predict(lm4_2, \n                                        newdata = predict4values)\n\n# Put the predicted scores back into our data set of possible values\npredict4values <- predict4values %>%\n  mutate(predicted_violent_crime_rate = predicted_violent_crime_rate)\n  \n\nprint(predict4values)  \n\n   per_child_poverty med_hh_income predicted_violent_crime_rate\n1                  9         60252                     180.7560\n2                 10         60252                     201.9358\n3                 11         60252                     223.1156\n4                 12         60252                     244.2953\n5                 13         60252                     265.4751\n6                 14         60252                     286.6549\n7                 15         60252                     307.8346\n8                 16         60252                     329.0144\n9                 17         60252                     350.1942\n10                18         60252                     371.3739\n11                19         60252                     392.5537\n12                20         60252                     413.7335\n13                21         60252                     434.9132\n14                22         60252                     456.0930\n15                23         60252                     477.2728\n16                24         60252                     498.4525\n17                25         60252                     519.6323\n18                26         60252                     540.8121\n19                27         60252                     561.9918\n20                28         60252                     583.1716\n\n\nNow graph it:\n\nprediction_graph <- ggplot(data = predict4values,\n                           mapping = aes(x = per_child_poverty,\n                                         y = predicted_violent_crime_rate)) +\n  geom_line(linewidth = 2) +\n  coord_cartesian(xlim = c(9, 28), ylim = c(100, 700)) +\n  scale_x_continuous(breaks = seq(10, 25, by = 5)) +\n  scale_y_continuous(breaks = seq(100, 700, by = 100)) +\n  theme_light() +\n  labs(x = \"Perecent Child Poverty\",\n       y = \"Predicted Violent Crime Rate (yhat)\")\n\nprint(prediction_graph)"
  },
  {
    "objectID": "chapter4.html#page-71-3d-plot",
    "href": "chapter4.html#page-71-3d-plot",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.5 Page 71 3D plot",
    "text": "4.5 Page 71 3D plot\nWhen you have two predictors you don’t have a regression line - you have a regression surface. The code below creates a surface plot that you can interact with/rotate/etc. Note that even though we put med_hh_income in the moderator position in the function there is no moderation here.\n\nlibrary(fastInteraction)\n\nsurface_plot <- fast.plot(lm4_2,\n                          criterion = violent_crime_rate,\n                          predictor = med_hh_income,\n                          moderator = per_child_poverty)\n\nsurface_plot"
  },
  {
    "objectID": "chapter4.html#page-72-understanding-b-weights",
    "href": "chapter4.html#page-72-understanding-b-weights",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.6 Page 72 Understanding b-weights",
    "text": "4.6 Page 72 Understanding b-weights"
  },
  {
    "objectID": "chapter4.html#page-76-understanding-b--vs-beta-weights",
    "href": "chapter4.html#page-76-understanding-b--vs-beta-weights",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.7 Page 76 Understanding b- vs beta-weights",
    "text": "4.7 Page 76 Understanding b- vs beta-weights"
  },
  {
    "objectID": "chapter4.html#page-77-relative-importance",
    "href": "chapter4.html#page-77-relative-importance",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.8 Page 77 Relative importance",
    "text": "4.8 Page 77 Relative importance"
  },
  {
    "objectID": "chapter4.html#page-79-predicted-means",
    "href": "chapter4.html#page-79-predicted-means",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.9 Page 79 Predicted Means",
    "text": "4.9 Page 79 Predicted Means"
  },
  {
    "objectID": "chapter4.html#page-86-chapter-exercises",
    "href": "chapter4.html#page-86-chapter-exercises",
    "title": "4  Multiple Linear Regression Models",
    "section": "4.10 Page 86 Chapter Exercises",
    "text": "4.10 Page 86 Chapter Exercises\n\n4.10.1 Activate packages\n\nlibrary(usethis) # use_github_file()\nlibrary(tidyverse) # read_csv() \nlibrary(janitor) # clean_names() \n\n\n\n4.10.2 Obtain data and save it to your computer\n\nuse_github_file(repo_spec = \"https://github.com/johnhoffmannVA/LinearRegression/blob/main/TeenBirths.csv\",\n                save_as = \"teenbirths.csv\")\n\n\n\n4.10.3 Load data from your computer\n\nteenbirths <- read_csv(\"teenbirths.csv\") %>% \n  clean_names()\n\n\n\n4.10.4 Inspect data\n\nteenbirths %>% \n  glimpse()  \n\nRows: 2,946\nColumns: 7\n$ state             <chr> \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama\", \"Alabama…\n$ county            <chr> \"Autauga\", \"Baldwin\", \"Barbour\", \"Bibb\", \"Blount\", \"…\n$ teen_birth_rate   <dbl> 51, 50, 74, 59, 51, 90, 66, 58, 70, 62, 61, 53, 46, …\n$ per_uninsured     <dbl> 14, 16, 19, 18, 18, 21, 18, 16, 19, 18, 19, 18, 17, …\n$ per_hsgrads       <dbl> 85, 73, 64, 67, 75, 58, 76, 72, 61, 67, 73, 68, 74, …\n$ per_child_poverty <dbl> 18, 20, 36, 29, 24, 40, 40, 32, 44, 33, 32, 30, 34, …\n$ per_singleparent  <dbl> 30, 29, 52, 35, 25, 64, 50, 39, 46, 26, 30, 37, 40, …\n\n\n\nteenbirths <- teenbirths %>% \n  mutate(state = as_factor(state)) %>%\n  mutate(county = as_factor(county))\n\n\nteenbirths %>% \n  glimpse()  \n\nRows: 2,946\nColumns: 7\n$ state             <fct> Alabama, Alabama, Alabama, Alabama, Alabama, Alabama…\n$ county            <fct> Autauga, Baldwin, Barbour, Bibb, Blount, Bullock, Bu…\n$ teen_birth_rate   <dbl> 51, 50, 74, 59, 51, 90, 66, 58, 70, 62, 61, 53, 46, …\n$ per_uninsured     <dbl> 14, 16, 19, 18, 18, 21, 18, 16, 19, 18, 19, 18, 17, …\n$ per_hsgrads       <dbl> 85, 73, 64, 67, 75, 58, 76, 72, 61, 67, 73, 68, 74, …\n$ per_child_poverty <dbl> 18, 20, 36, 29, 24, 40, 40, 32, 44, 33, 32, 30, 34, …\n$ per_singleparent  <dbl> 30, 29, 52, 35, 25, 64, 50, 39, 46, 26, 30, 37, 40, …\n\n\n\nteenbirths %>% \n  skim()  \n\n\nData summary\n\n\nName\nPiped data\n\n\nNumber of rows\n2946\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nstate\n0\n1\nFALSE\n50\nTex: 238, Geo: 158, Vir: 128, Ken: 119\n\n\ncounty\n0\n1\nFALSE\n1744\nWas: 31, Jef: 26, Fra: 24, Jac: 23\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nteen_birth_rate\n0\n1\n47.44\n20.56\n4\n31\n46\n61\n127\n▃▇▅▂▁\n\n\nper_uninsured\n0\n1\n18.30\n5.75\n3\n14\n18\n22\n43\n▂▇▆▁▁\n\n\nper_hsgrads\n0\n1\n82.08\n9.67\n14\n77\n83\n89\n100\n▁▁▁▆▇\n\n\nper_child_poverty\n0\n1\n24.44\n9.10\n3\n18\n24\n30\n61\n▂▇▅▁▁\n\n\nper_singleparent\n0\n1\n31.14\n9.85\n4\n25\n30\n36\n76\n▁▇▅▁▁"
  }
]